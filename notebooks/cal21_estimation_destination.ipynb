{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # for automation and parallelisation\n",
    "manual, scenario = (True, 'base') if 'ipykernel' in sys.argv[0] else (False, sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "import biogeme.models as models\n",
    "import biogeme.optimization as opt\n",
    "import biogeme.messaging as message\n",
    "from biogeme import expressions as ex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlsxwriter\n",
    "from tqdm import tqdm\n",
    "from quetzal.io import excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration\n",
    "## Estimation of the model parameters\n",
    "quetzal_germany is being estimated using [PandasBiogeme](https://biogeme.epfl.ch/). This notebook estimates calibration parameters for the model's utility functions.\n",
    "- Documentation and reference: [Bierlaire, M. (2020). A short introduction to PandasBiogeme. Technical report TRANSP-OR 200605. Transport and Mobility Laboratory, ENAC, EPFL.](https://transp-or.epfl.ch/documents/technicalReports/Bier20.pdf)\n",
    "- Tutorial: https://www.youtube.com/watch?v=OiM94B8WayA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model formulation\n",
    "\n",
    "The joint generation and distribution model makes distance classes in destination choice obsolete. It accounts for each possible destination zone plus a stay option (not making any trip).\n",
    "\n",
    "$V_i^{zone} = V_{i,m}^{mode} + log(\\sum_{j\\in A} \\beta_j a_{i,j}) + \\beta_{TOUR} a_{i,TOUR}$\n",
    "\n",
    "with $\\beta_1=1$. Zone-specific attraction attributes are covered in theory in Daly (1982; DOI: 10.1016/0191-2615(82)90037-6):\n",
    "\n",
    "$A = \\text{\\{D_POP, D_EMPL, D_SHOP, D_EDU, D_LEISURE, D_TOURISM\\}}$\n",
    "\n",
    "In the choice tree, each zone has a nest of modes to reach it. In case of sequential estimation, results of the mode tier become aggregated with the formula of generalised cost. The mode choice model consists of systematic utility functions, one for each mode in the choice set. They comprise an alternaive-specific constant (ASC), a distance-dependent part with travel time and cost summarised as generalised cost (GC), and a cost damping function F\n",
    "\n",
    "$V_{i,m}^{mode} = ASC_{i,m} + F(GC(T_m, C_m)_i, b_{gc_i})$\n",
    "\n",
    "Index i marks the demand group. I = {'commuting' (1), 'education' (2), 'shopping/medical' (3), 'business' (4), 'private' (6)} x {'no_car' (0), 'car_owned' (1)}\n",
    "\n",
    "Note: The cost variable already includes subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../input/'\n",
    "model_path = '../model/'\n",
    "output_path = '../output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters for settings\n",
    "params = excel.read_var(file='../input/parameters.xls', scenario=scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_path + 'transport_demand/calibration_all_trips_MiD2017.csv')\n",
    "df = df[['cost_rail_short', 'cost_rail_long', 'cost_car', 'cost_coach', 'cost_bus', 'cost_walk', 'cost_air',\n",
    "         'time_rail_short', 'time_rail_long', 'time_car', 'time_coach', 'time_bus', 'time_walk', 'time_air',\n",
    "         'cost_rail', 'cost_road', 'time_rail', 'time_road',\n",
    "         'mode_model', 'purpose_model', 'purpose2', 'car_avail', 'distance', 'origin', 'destination', 'P_ID', 'W_GEW',\n",
    "         'dest_urban', 'dest_pop', 'dest_employment', 'dest_tourism', 'dest_area',\n",
    "         'dest_shopping_many', 'dest_shopping_few', 'dest_education_many', 'dest_education_few',\n",
    "         'dest_leisure_many', 'dest_leisure_few', 'dest_accompany']]\n",
    "df.columns = ['C_RAIL_S', 'C_RAIL_L', 'C_CAR', 'C_COACH', 'C_BUS', 'C_NON_MOTOR', 'C_AIR',\n",
    "              'T_RAIL_S', 'T_RAIL_L', 'T_CAR', 'T_COACH', 'T_BUS', 'T_NON_MOTOR', 'T_AIR',\n",
    "              'C_RAIL', 'C_ROAD', 'T_RAIL', 'T_ROAD',\n",
    "              'MODE', 'PURPOSE', 'P2', 'CAR_AV', 'DIST', 'O', 'D', 'P_ID', 'W_GEW',\n",
    "              'D_URBAN', 'D_POP', 'D_EMPL', 'D_TOURISM', 'D_AREA',\n",
    "              'D_SHOP-MANY', 'D_SHOP-FEW', 'D_EDU-MANY', 'D_EDU-FEW',\n",
    "              'D_LEISURE-MANY', 'D_LEISURE-FEW', 'D_ACCOM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155235"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop return trips\n",
    "df = df.loc[~df['P2'].isin([8,9])]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The estimation requires numerical purpose values\n",
    "p_model_dict = {'commuting':1, 'business':2, 'education':3,\n",
    "                'buy/execute':4, 'leisure':6, 'accompany':7}\n",
    "df['PURPOSE'] = df['PURPOSE'].apply(lambda s: p_model_dict[s.split('_')[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = 10000\n",
    "df = df.replace({np.inf:inf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3631540267401566"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale population and employment\n",
    "df['D_POP_S'] = df['D_POP'] / 1e6\n",
    "df['D_EMPL_S'] = df['D_EMPL'] / 1e6\n",
    "df['D_POP_S'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale POIs to the same level as population\n",
    "df['D_SHOP-FEW_S'] = df['D_SHOP-FEW'] / 100\n",
    "df['D_EDU-FEW_S'] = df['D_EDU-FEW'] / 100\n",
    "df['D_LEISURE-FEW_S'] = df['D_LEISURE-FEW'] / 100\n",
    "df['D_SHOP-MANY_S'] = df['D_SHOP-MANY'] / 1000\n",
    "df['D_EDU-MANY_S'] = df['D_EDU-MANY'] / 1000\n",
    "df['D_LEISURE-MANY_S'] = df['D_LEISURE-MANY'] / 1000\n",
    "df['D_ACCOM_S'] = df['D_ACCOM'] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make car availability binary\n",
    "df['CAR_AV'] = df['CAR_AV'].replace({9:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PT availabilities\n",
    "df['RAIL_AV'] = (df['T_RAIL']!=inf).astype(int)\n",
    "df['RAIL_S_AV'] = (df['T_RAIL_S']!=inf).astype(int)\n",
    "df['RAIL_L_AV'] = (df['T_RAIL_L']!=inf).astype(int)\n",
    "df['COACH_AV'] = (df['T_COACH']!=inf).astype(int)\n",
    "df['BUS_AV'] = (df['T_BUS']!=inf).astype(int)\n",
    "df['ROAD_AV'] = (df['T_ROAD']!=inf).astype(int)\n",
    "df['AIR_AV'] = (df['T_AIR']!=inf).astype(int)\n",
    "df['NON_MOTOR_AV'] = (df['DIST']<100).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MODE'] = df['MODE'].replace({2:1, 3:4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash origin and destination columns (integer)\n",
    "zone_set = set(df['D']).union(set(df['O']))\n",
    "zone_dict = dict(zip(zone_set, range(len(zone_set))))\n",
    "df['O'] = df['O'].map(zone_dict).astype(int)\n",
    "df['D'] = df['D'].map(zone_dict).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance classes\n",
    "\n",
    "Destination choice does not work for a large set of choice options, such as 400 zones. Thus, the individual choice sets are limited to various distance classes, which are consistent with the stay/go and distance choice model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155235"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean distances\n",
    "df = df.loc[df['DIST']<=1000]\n",
    "df['DIST'] = df['DIST'].astype(int)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove STAY rows for estimation without gneration\n",
    "if 'STAY' in zone_dict.keys():\n",
    "    df = df.loc[df['D']!=zone_dict['STAY']]\n",
    "    print(len(df))\n",
    "else:\n",
    "    zone_dict['STAY'] = len(zone_set)+1\n",
    "    stay_mask = df['MODE']!=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify into distance bins\n",
    "bins = [-1, 60, 150, 1000]\n",
    "df['DIST_bin'] = pd.cut(df['DIST'], bins=bins, labels=bins[1:]).astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Origin-destination distances in MiD are very random and do not\n",
    "# neccessarily represent reality. We can obtain the most reported\n",
    "# distance class as most probable distance class for every OD pair.\n",
    "# Save distance classes into a dict by OD pair\n",
    "dist_dict = df.groupby(['O', 'D'])['DIST_bin'].value_counts().unstack().idxmax(axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Now, we can drop observations with wrong distances or OD pairs\n",
    "# which also affected LoS attributes in the previous step\n",
    "df['DIST_bin'] = [dist_dict[a] for a in tuple(zip(df['O'], df['D']))]\n",
    "df = df.loc[df['DIST_bin']==pd.cut(df['DIST'], bins=bins, labels=bins[1:])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add availability columns (based on distance classes)\n",
    "for zone in tqdm(df['D'].unique()):\n",
    "    df['AV_'+str(zone)] = [dist_dict[(o,zone)] == dist_bin\n",
    "                           if (o,zone) in dist_dict.keys() else False\n",
    "                           for o,dist_bin in zip(df['O'], df['DIST_bin'])]\n",
    "    df['AV_'+str(zone)] = df['AV_'+str(zone)].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DIST_bin\n",
       "1       101174\n",
       "60       47749\n",
       "150       3966\n",
       "1000      2346\n",
       "Name: O, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bin for STAY observations and inner-zonal trips\n",
    "df.loc[df['D']==zone_dict['STAY'], 'DIST_bin'] = 0\n",
    "df.loc[df['D']==df['O'], 'DIST_bin'] = 1\n",
    "df.groupby('DIST_bin').count()['O']#.plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip is inner- or inter-zonal?\n",
    "df['GO'] = df['DIST_bin'].clip(lower=0, upper=2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model variables\n",
    "All columns are variables. DefineVariable creates a new column in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost damping\n",
    "\n",
    "Many modelling studies have shown that cost damping is required in order to flatten the tail of time/cost elasticities, i.e. decrease the impact of long distances on choice results to prevent from overestimation of time/cost parameters. Cost damping represents the property of decreasing marginal utility. It is commonly approached with Box-Cox transformations of generalised cost (usually defined as the sum of travel time and travel expenditures divided by the value of time). Daly (2010) proposes a hybrid function as a sum of the linear term and the narural logarithm of the same. However, the linear term still dominates cost on long distances. Rich (2015), main developer of the Danish National Transport Model, proposes a more complex spline function which successfully manages cost damping and even outperforms the Box-Cox transformation in terms of stability of elasticities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cost damping function from Rich (2020)\n",
    "c = params['estimation']\n",
    "def spline(x, beta, c1, c2, Q=3):\n",
    "    alpha = [0, -beta/2*ex.Power(ex.log(c1),3),\n",
    "             -beta/2*ex.log(c1)*(3*ex.Power(ex.log(c2),2)+ex.Power(ex.log(c1),2))] # for Q=3\n",
    "    theta = [1, 3/2*ex.log(c1), 3*ex.log(c1)*ex.log(c2)] # for Q=3\n",
    "    return (beta*theta[0]*ex.Power(ex.log(x),Q-1+1) + alpha[0]) * (x<c1) \\\n",
    "    + (beta*theta[1]*ex.Power(ex.log(x),Q-2+1) + alpha[1]) * (x>=c1)*(x<c2) \\\n",
    "    + (beta*theta[2]*ex.Power(ex.log(x),Q-3+1) + alpha[2]) * (x>=c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalised cost\n",
    "\n",
    "Both, travel time and monetary cost should be affected by cost damping measures. It is logical to define a generalised cost term `GC` with dimension of time units. This requires definition or estimation of values of time, in order to rescale monetary units, for all segments. Usually, the value of time (VoT) is distance-dependent. In the case of Germany, VoT can be taken from research undertaken within the Federal Government's transport study \"Bundesverkehrswegeplan 2030\": Axhausen et al. 2015. Ermittlung von Bewertungsansätzen für Reisezeiten und Zuverlässigkeit auf der Basis eines Modells für modale Verlagerungen im nicht-gewerblichen und gewerblichen Personenverkehr für die Bundesverkehrswegeplanung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VoT for GC calculation\n",
    "vot = pd.read_csv(input_path + 'vot.csv', header=[0,1], index_col=0).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation of purposes (MiD2017 to VP2030)\n",
    "p_dict = {1:'Fz1', 3:'Fz2', 4:'Fz3', 2:'Fz4', 6:'Fz6', 7:'Fz6'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the GC function\n",
    "car_vot_descr = ['PT', 'all'] # for indexing with car=[0,1]\n",
    "def GC(time, price, p, car, dist):\n",
    "    return time + (price / vot[(p, car_vot_descr[car])][dist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility\n",
    "\n",
    "The database should have columns with prices and travel times for each mode of the trip which can be used to calculate utilities of mode choice, given the estimated beta parameters from the mode choice step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mode choice parameters\n",
    "beta_time = {} # time in minutes\n",
    "beta_price = {}\n",
    "excel = pd.ExcelFile(input_path + 'estimation_results.xls')\n",
    "p_dict_model = {1:'commuting', 2:'business', 3:'education', 4:'buy/execute', 6:'leisure', 7:'accompany'}\n",
    "for p, p_str in p_dict_model.items():\n",
    "    beta_time[p] = {}\n",
    "    beta_price[p] = {}\n",
    "    for car, car_str in {1:'_car', 0:'_no_car'}.items():\n",
    "        params_est = excel.parse(p_str.replace('/', '-')+car_str, index_col=0)\n",
    "        beta_time[p][car] = params_est.loc['b_t', 'Value']\n",
    "        try:\n",
    "            beta_price[p][car] = params_est.loc['b_c', 'Value']\n",
    "        except KeyError:\n",
    "            beta_price[p][car] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get utility of travel time\n",
    "def time_ut(time, p, car):\n",
    "    # Given time in minutes and the segment (purpose, car)\n",
    "    #return spline(time, beta_time[p][car], c['c1_time_'+str(p_dict[p][-1])]*60, c['c2_time_'+str(p_dict[p][-1])]*60)\n",
    "    # The fist part of the spline is a log-power\n",
    "    return np.power(np.log(time), 3) * beta_time[p][car]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get utility of price\n",
    "def price_ut(price, p, car):\n",
    "    # Given the price and the segment (purpose, car)\n",
    "    return price * beta_price[p][car]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composite cost / logsum\n",
    "In a sequential estimation process, it is required to aggregate all variables of the upper decision level and insert them in this decision level. Using the formula from Walker (1977) - see formula 6.10 in Ortúzar and Willumsen (2011) p. 213 - we can assume that the perception of alternatives is depicted properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inter-zonal composite cost from mode choice step\n",
    "cc = pd.read_csv(output_path + scenario + '/mode_choice_od_composite_cost.csv')\n",
    "# Map to integer zone names and drop unused pairs\n",
    "cc['origin'] = cc['origin'].map(zone_dict)\n",
    "cc['destination'] = cc['destination'].map(zone_dict)\n",
    "cc = cc.loc[(cc['origin'].notna()) & (cc['destination'].notna())]\n",
    "cc['origin'] = cc['origin'].astype(int)\n",
    "cc['destination'] = cc['destination'].astype(int)\n",
    "cc.set_index(['origin', 'destination'], inplace=True)\n",
    "cc.index.names = ['O', 'D']\n",
    "# Rename segments to integer values\n",
    "cc.columns = pd.MultiIndex.from_tuples(\n",
    "    [(p_model_dict[seg.split('_')[0]], {'no': 0, 'car': 1}[seg.split('_')[1]])\n",
    "     for seg in cc.columns],\n",
    "    names=['PURPOSE', 'CAR_AV'])\n",
    "# Reshape the table into a mergable format\n",
    "cc = cc.unstack('D').stack('PURPOSE').stack('CAR_AV')\n",
    "cc.columns = ['CC_'+str(zone) for zone in cc.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inter-zonal CC\n",
    "df = df.merge(cc, how='left', left_on=['O', 'PURPOSE', 'CAR_AV'], right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1933/1933 [00:47<00:00, 40.70it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inner-zonal\n",
    "for d in tqdm(list(zone_dict.values())):\n",
    "    mask = df['O']!=d\n",
    "    car_av_s = df.loc[~mask, 'CAR_AV']\n",
    "    p_s = df.loc[~mask, 'PURPOSE']\n",
    "    t_rail_s = pd.Series([time_ut(t, p, car) if t!=inf else np.nan\n",
    "                          for t,p,car in zip(df.loc[~mask, 'T_RAIL'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    t_bus_s = pd.Series([time_ut(t, p, car) if t!=inf else np.nan\n",
    "                         for t,p,car in zip(df.loc[~mask, 'T_BUS'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    t_car_s = pd.Series([time_ut(t, p, car) if car==1 else np.nan\n",
    "                         for t,p,car in zip(df.loc[~mask, 'T_CAR'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    t_non_s = pd.Series([time_ut(t, p, car) if t!=inf else np.nan\n",
    "                         for t,p,car in zip(df.loc[~mask, 'T_NON_MOTOR'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    p_rail_s = pd.Series([price_ut(t, p, car) if t!=inf else np.nan\n",
    "                          for t,p,car in zip(df.loc[~mask, 'C_RAIL'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    p_bus_s = pd.Series([price_ut(t, p, car) if t!=inf else np.nan\n",
    "                         for t,p,car in zip(df.loc[~mask, 'C_BUS'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    p_car_s = pd.Series([price_ut(t, p, car) if car==1 else np.nan\n",
    "                         for t,p,car in zip(df.loc[~mask, 'C_CAR'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    # Calculate the logsum\n",
    "    df.loc[~mask, 'CC_'+str(d)] = [np.log(np.maximum(4, np.sum(\n",
    "        pd.Series([np.exp(-1*(t_rail+p_rail)),\n",
    "                   np.exp(-1*(t_bus+p_bus)),\n",
    "                   np.exp(-1*(t_car+p_car)),\n",
    "                   np.exp(-1*t_non)]))))\n",
    "                                   for t_rail, t_bus, t_car, t_non, p_rail, p_bus, p_car\n",
    "                                   in zip(t_rail_s, t_bus_s, t_car_s, t_non_s,\n",
    "                                          p_rail_s, p_bus_s, p_car_s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with segment's max values\n",
    "for p in df['PURPOSE'].unique():\n",
    "    for car in df['CAR_AV'].unique():\n",
    "        mask = (df['PURPOSE']==p) & (df['CAR_AV']==car), [c for c in df.columns if c[:2]=='CC']\n",
    "        df.loc[mask] = df.loc[mask].fillna(\n",
    "            cc.xs(p, level='PURPOSE').xs(car, level='CAR_AV').max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All CC must be greater 0\n",
    "assert df[[c for c in df.columns if c.startswith('CC_')]].min().min() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accessibility of the origin zone\n",
    "# Use the logsum formula with CC from all other zones\n",
    "#df['ACC'] = np.log(np.sum(np.exp(df[[c for c in df.columns if c.startswith('CC_')]]), axis=1))\n",
    "df['ACC'] = df[[c for c in df.columns if c.startswith('CC_')]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['ACC'].hist(bins=100, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155221"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add distances to the attributes\n",
    "# in case CC does not suffice as distance measure\n",
    "dist = pd.read_csv(output_path + 'distances_centroids.csv') # km\n",
    "dist = dist.loc[(dist['origin'].map(zone_dict).notna())\n",
    "                & (dist['destination'].map(zone_dict).notna())]\n",
    "dist['origin'] = dist['origin'].map(zone_dict).astype(int)\n",
    "dist['destination'] = dist['destination'].map(zone_dict).astype(int)\n",
    "dist.set_index(['origin', 'destination'], inplace=True)\n",
    "dist = dist.unstack('destination').fillna(0).droplevel(0, axis=1)\n",
    "df = df.merge(dist, how='left', left_on='O', right_index=True)\n",
    "df.rename(columns={z: 'DIST_'+str(z) for z in df['D'].unique()}, inplace=True)\n",
    "df = df.loc[df.notna().all(axis=1)]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zone attraction\n",
    "\n",
    "As for modes, every observation needs attraction attributes for all zones available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate zone-specific variables for every attribute\n",
    "for attr in ['D_URBAN', 'D_POP', 'D_EMPL', 'D_AREA'#,\n",
    "             #'D_SHOP-MANY', 'D_EDU-MANY', 'D_LEISURE-MANY',\n",
    "             #'D_SHOP-FEW', 'D_EDU-FEW', 'D_LEISURE-FEW', 'D_ACCOM'\n",
    "             #'D_SHOP-MANY_S', 'D_EDU-MANY_S', 'D_LEISURE-MANY_S',\n",
    "             #'D_SHOP-FEW_S', 'D_EDU-FEW_S', 'D_LEISURE-FEW_S', 'D_ACCOM_S'\n",
    "            ]:\n",
    "    attr_dict = df.set_index('D')[attr].to_dict()\n",
    "    for zone, value in attr_dict.items():\n",
    "        globals()[attr.split('_')[1] + '_' + str(zone)] = value\n",
    "    globals()[attr.split('_')[1] + '_' + str(zone_dict['STAY'])] = 0\n",
    "    df[attr.replace('D_', 'O_').replace('-', '_')] = df['O'].map(attr_dict).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHILDCARE\n",
      "DAILY_LEISURE\n",
      "HIGHER_EDUCATION\n",
      "HOLIDAY\n",
      "MEDICAL\n",
      "OCCASIONAL_LEISURE\n",
      "SCHOOL\n",
      "SHOP\n",
      "SPECIAL_SHOP\n"
     ]
    }
   ],
   "source": [
    "# Generate categories of OSM POIs\n",
    "cats = pd.read_excel(input_path + 'spatial_OSM_POI_list.xlsx', sheet_name='categories')\n",
    "pois = pd.read_csv(input_path + 'spatial_num_pois_raw.csv', index_col='index')\n",
    "cats['label'] = (cats['key'] + ' ' + cats['value'].fillna('')).str.strip()\n",
    "col_dict = cats.loc[cats['category'].notna()\n",
    "                   ].groupby('category').agg({'label': list}).to_dict()['label']\n",
    "for category, columns in col_dict.items():\n",
    "    for zone, value in pois[columns].sum(axis=1).items():\n",
    "        if zone in zone_dict.keys():\n",
    "            globals()[category.upper() + '_' + str(zone_dict[zone])] = value\n",
    "    globals()[category.upper() + '_' + str(zone_dict['STAY'])] = 0\n",
    "    df['O_'+category.upper()] = df['O'].map(attr_dict).fillna(0)\n",
    "    print(category.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create database\n",
    "\n",
    "This makes all columns become global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155221"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the initial database and make columns global variables\n",
    "database = db.Database('MiD2017', df.copy())\n",
    "globals().update(database.variables)\n",
    "database.getSampleSize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination choice / attraction\n",
    "b_urban = ex.Beta('b_urban', 0, None, None, 0)\n",
    "b_pop = ex.Beta('b_pop', 0, None, None, 0)\n",
    "b_empl = ex.Beta('b_empl', 0.1, 0, None, 0)\n",
    "b_shop1 = ex.Beta('b_shop1', 0.1, 0, None, 0)\n",
    "b_edu1 = ex.Beta('b_edu1', 0.1, 0, None, 0)\n",
    "b_leisure1 = ex.Beta('b_leisure1', 0.1, 0, None, 0)\n",
    "b_accom1 = ex.Beta('b_accom1', 0.1, 0, None, 0)\n",
    "b_shop2 = ex.Beta('b_shop2', 0.1, 0, None, 0)\n",
    "b_edu2 = ex.Beta('b_edu2', 0.1, 0, None, 0)\n",
    "b_leisure2 = ex.Beta('b_leisure2', 0.1, 0, None, 0)\n",
    "b_accom2 = ex.Beta('b_accom2', 0.1, 0, None, 0)\n",
    "b_shop3 = ex.Beta('b_shop3', 0.1, 0, None, 0)\n",
    "b_edu3 = ex.Beta('b_edu3', 0.1, 0, None, 0)\n",
    "b_leisure3 = ex.Beta('b_leisure3', 0.1, 0, None, 0)\n",
    "b_accom3 = ex.Beta('b_accom3', 0.1, 0, None, 0)\n",
    "b_local = ex.Beta('b_local', 0, None, None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta parameter for size variables, following Daly 1982\n",
    "b_size = ex.Beta('b_size', 0, 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalised cost function / mode choice utility parameter\n",
    "b_gc = ex.Beta('b_gc', -0.1, None, 0, 0)\n",
    "# Distance parameter\n",
    "b_dist = ex.Beta('b_dist', -0.1, None, 0, 0)\n",
    "b_dist2 = ex.Beta('b_dist2', -0.1, None, None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logsum parameters\n",
    "phi = ex.Beta('phi', 0.5, 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for nests\n",
    "mu_go = ex.Beta('mu_go', 1, 1, 10, 0)\n",
    "mu_u1 = ex.Beta('mu_u1', 1, 1, 10, 0)\n",
    "mu_u2 = ex.Beta('mu_u2', 1, 1, 10, 0)\n",
    "mu_u3 = ex.Beta('mu_u3', 1, 1, 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASC for stay/go choice\n",
    "asc_stay = ex.Beta('asc_stay', 0, None, None, 0)\n",
    "asc_inner = ex.Beta('asc_inner', 0, None, None, 0)\n",
    "asc_inter = ex.Beta('asc_inter', 0, None, None, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Destination choice utility by segment\n",
    "# using the sum of logarithms of variables of size\n",
    "# sumOfLogs\n",
    "V_D = {p: {} for p in df['PURPOSE'].unique()}\n",
    "for car in df['CAR_AV'].unique():\n",
    "    # Commuting\n",
    "    V_D[1][car] = {zone: ex.log(globals()['EMPL_'+str(zone)]) * b_empl\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}\n",
    "    ## Business\n",
    "    V_D[2][car] = {zone: ex.log(globals()['EMPL_'+str(zone)]) * b_empl\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}\n",
    "    # Education\n",
    "    V_D[3][car] = {zone: ex.log(1 + globals()['HIGHER_EDUCATION_'+str(zone)]) * b_edu1\n",
    "           + ex.log(1 + globals()['SCHOOL_'+str(zone)]) * b_edu2\n",
    "           + ex.log(1 + globals()['CHILDCARE_'+str(zone)]) * b_edu3\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}\n",
    "    # Shopping\n",
    "    V_D[4][car] = {zone: ex.log(1 + globals()['MEDICAL_'+str(zone)]) * b_shop1\n",
    "           + ex.log(1 + globals()['SHOP_'+str(zone)]) * b_shop2\n",
    "           + ex.log(1 + globals()['SPECIAL_SHOP_'+str(zone)]) * b_shop3\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}\n",
    "    # Leisure\n",
    "    V_D[6][car] = {zone: ex.log(1 + globals()['HOLIDAY_'+str(zone)]) * b_leisure1\n",
    "           + ex.log(1 + globals()['DAILY_LEISURE_'+str(zone)]) * b_leisure2\n",
    "           + ex.log(1 + globals()['OCCASIONAL_LEISURE_'+str(zone)]) * b_leisure3\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}\n",
    "    # Accompany\n",
    "    V_D[7][car] = {zone: ex.log(1 + globals()['MEDICAL_'+str(zone)]) * b_accom1\n",
    "           + ex.log(1 + globals()['SCHOOL_'+str(zone)]) * b_accom2\n",
    "           + ex.log(1 + globals()['CHILDCARE_'+str(zone)]) * b_accom3\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination choice utility by segment\n",
    "# using the sum of variables of size in a logarithm\n",
    "# Daly3\n",
    "V_D = {}\n",
    "for p in df['PURPOSE'].unique():\n",
    "    V_D[p] = {}\n",
    "for car in df['CAR_AV'].unique():\n",
    "    # Commuting\n",
    "    V_D[1][car] = {zone: ex.log(globals()['EMPL_'+str(zone)]) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           for zone in df['D'].unique()}\n",
    "    ## Business\n",
    "    V_D[2][car] = {zone: ex.log(globals()['EMPL_'+str(zone)]) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           + globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           for zone in df['D'].unique()}\n",
    "    # Education\n",
    "    V_D[3][car] = {zone: ex.log(1 + globals()['CHILDCARE_'+str(zone)] * ex.exp(b_edu1)\n",
    "                                + globals()['SCHOOL_'+str(zone)] * ex.exp(b_edu2)\n",
    "                                + globals()['HIGHER_EDUCATION_'+str(zone)]\n",
    "                               ) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)] )/ globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           for zone in df['D'].unique()}\n",
    "    # Shopping\n",
    "    V_D[4][car] = {zone: ex.log(1 + globals()['SHOP_'+str(zone)] * ex.exp(b_shop1)\n",
    "                                + globals()['MEDICAL_'+str(zone)] * ex.exp(b_shop2)\n",
    "                                + globals()['SPECIAL_SHOP_'+str(zone)]\n",
    "                               ) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           for zone in df['D'].unique()}\n",
    "    # Leisure\n",
    "    V_D[6][car] = {zone: ex.log(1 + globals()['DAILY_LEISURE_'+str(zone)] * ex.exp(b_leisure1)\n",
    "                                + globals()['HOLIDAY_'+str(zone)] * ex.exp(b_leisure2)\n",
    "                                + globals()['OCCASIONAL_LEISURE_'+str(zone)]\n",
    "                               ) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           + globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           + globals()['DIST_'+str(zone)] * globals()['DIST_'+str(zone)] * b_dist2 * int(car>0)\n",
    "           for zone in df['D'].unique()}\n",
    "    # Accompany\n",
    "    V_D[7][car] = {zone: ex.log(1 + globals()['MEDICAL_'+str(zone)] * ex.exp(b_accom1)\n",
    "                                + globals()['SCHOOL_'+str(zone)] * ex.exp(b_accom2)\n",
    "                                + globals()['CHILDCARE_'+str(zone)]\n",
    "                               ) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           + globals()['DIST_'+str(zone)] * globals()['DIST_'+str(zone)] * b_dist2\n",
    "           for zone in df['D'].unique()}\n",
    "\n",
    "# The utility for STAY is zero\n",
    "for p in V_D.keys():\n",
    "    for car in [0,1]:\n",
    "        V_D[p][car][zone_dict['STAY']] = 0 #asc_stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'O_EDU_MANY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [46]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m V_GO[\u001b[38;5;241m2\u001b[39m][car][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m O_EMPL) \u001b[38;5;241m*\u001b[39m b_empl \\\n\u001b[0;32m     17\u001b[0m        \u001b[38;5;241m+\u001b[39m ex\u001b[38;5;241m.\u001b[39mlog((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m O_POP) \u001b[38;5;241m/\u001b[39m O_AREA) \u001b[38;5;241m*\u001b[39m b_pop \\\n\u001b[0;32m     18\u001b[0m        \u001b[38;5;241m+\u001b[39m asc_inner\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Education\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m V_GO[\u001b[38;5;241m3\u001b[39m][car][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mO_EDU_MANY\u001b[49m) \u001b[38;5;241m*\u001b[39m b_edu1 \\\n\u001b[0;32m     21\u001b[0m        \u001b[38;5;241m+\u001b[39m ex\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m O_EDU_FEW) \u001b[38;5;241m*\u001b[39m b_edu2 \\\n\u001b[0;32m     22\u001b[0m        \u001b[38;5;241m+\u001b[39m ex\u001b[38;5;241m.\u001b[39mlog((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m O_POP) \u001b[38;5;241m/\u001b[39m O_AREA) \u001b[38;5;241m*\u001b[39m b_pop \\\n\u001b[0;32m     23\u001b[0m        \u001b[38;5;241m+\u001b[39m asc_inner\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Shopping\u001b[39;00m\n\u001b[0;32m     25\u001b[0m V_GO[\u001b[38;5;241m4\u001b[39m][car][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m O_SHOP_MANY) \u001b[38;5;241m*\u001b[39m b_shop1 \\\n\u001b[0;32m     26\u001b[0m        \u001b[38;5;241m+\u001b[39m ex\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m O_SHOP_FEW) \u001b[38;5;241m*\u001b[39m b_shop2 \\\n\u001b[0;32m     27\u001b[0m        \u001b[38;5;241m+\u001b[39m ex\u001b[38;5;241m.\u001b[39mlog((\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m O_POP) \u001b[38;5;241m/\u001b[39m O_AREA) \u001b[38;5;241m*\u001b[39m b_pop \\\n\u001b[0;32m     28\u001b[0m        \u001b[38;5;241m+\u001b[39m asc_inner\n",
      "\u001b[1;31mNameError\u001b[0m: name 'O_EDU_MANY' is not defined"
     ]
    }
   ],
   "source": [
    "# The utility for STAY/GO\n",
    "V_GO = {}\n",
    "for p in df['PURPOSE'].unique():\n",
    "    V_GO[p] = {car: {} for car in df['CAR_AV'].unique()}\n",
    "    for car in df['CAR_AV'].unique():\n",
    "        V_GO[p][car][0] = 0 #asc_stay\n",
    "        V_GO[p][car][2] = asc_inter + ACC * b_gc\n",
    "\n",
    "# Utility for inner-zonal travel depends on zone POIs\n",
    "for car in df['CAR_AV'].unique():\n",
    "    # Commuting\n",
    "    V_GO[1][car][1] = ex.log(1 + O_EMPL) * b_empl \\\n",
    "           + ex.log((1 + O_POP) / O_AREA) * b_pop \\\n",
    "           + asc_inner\n",
    "    ## Business\n",
    "    V_GO[2][car][1] = ex.log(1 + O_EMPL) * b_empl \\\n",
    "           + ex.log((1 + O_POP) / O_AREA) * b_pop \\\n",
    "           + asc_inner\n",
    "    # Education\n",
    "    V_GO[3][car][1] = ex.log(1 + O_EDU_MANY) * b_edu1 \\\n",
    "           + ex.log(1 + O_EDU_FEW) * b_edu2 \\\n",
    "           + ex.log((1 + O_POP) / O_AREA) * b_pop \\\n",
    "           + asc_inner\n",
    "    # Shopping\n",
    "    V_GO[4][car][1] = ex.log(1 + O_SHOP_MANY) * b_shop1 \\\n",
    "           + ex.log(1 + O_SHOP_FEW) * b_shop2 \\\n",
    "           + ex.log((1 + O_POP) / O_AREA) * b_pop \\\n",
    "           + asc_inner\n",
    "    # Leisure\n",
    "    V_GO[6][car][1] = ex.log(1 + O_LEISURE_MANY) * b_leisure1 \\\n",
    "           + ex.log(1 + O_LEISURE_FEW) * b_leisure2 \\\n",
    "           + ex.log(1 + O_POP / O_AREA) * b_pop \\\n",
    "           + asc_inner\n",
    "    # Accompany\n",
    "    V_GO[7][car][1] = ex.log(1 + O_ACCOM) * b_accom1 \\\n",
    "           + ex.log(1 + O_POP / O_AREA) * b_pop \\\n",
    "           + asc_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define level of verbosity\n",
    "logger = message.bioMessage()\n",
    "#logger.setSilent()\n",
    "logger.setWarning()\n",
    "#logger.setGeneral()\n",
    "#logger.setDetailed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urbanisation nests\n",
    "nest_u1 = mu_u1, list(set(df.loc[df['D_URBAN']==1, 'D']))\n",
    "nest_u2 = mu_u2, list(set(df.loc[df['D_URBAN']==2, 'D']))\n",
    "nest_u3 = mu_u3, list(set(df.loc[df['D_URBAN']==3, 'D']))\n",
    "stay = 1, [zone_dict['STAY']]\n",
    "urban_nests = nest_u1, nest_u2, nest_u3, stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stay/go nests\n",
    "stay = 1, [zone_dict['STAY']]\n",
    "go = mu_go, [d for d in zone_dict.values() if d!=zone_dict['STAY']]\n",
    "go_nest = stay, go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# TEST DATABASE\n",
    "mask = (df['PURPOSE']==3)\n",
    "database = db.Database('MiD2017', pd.concat([\n",
    "    df.loc[(df['D']!=zone_dict['STAY']) & (df['O']!=df['D']) & mask].sample(100) # inter-zonal trips\n",
    "    #df.loc[(df['D']!=zone_dict['STAY']) & (df['O']==df['D']) & mask].sample(500) # inner-zonal trips\n",
    "    #df.loc[(df['D']==zone_dict['STAY']) & mask].sample(300)]) # stay observations\n",
    "]))\n",
    "print(database.getSampleSize())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TEST estimation with Nested Logit\n",
    "nl = models.lognested(V_D[3][1], None, urban_nests, D)\n",
    "model_nl = bio.BIOGEME(database, nl)\n",
    "model_nl.modelName = 'NL'\n",
    "estimated_model = model_nl.estimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST  estimation MNL\n",
    "mnl = models.loglogit(V_D[3][1], None, D)\n",
    "model_mnl = bio.BIOGEME(database, mnl)\n",
    "model_mnl.modelName = 'MNL'\n",
    "estimated_model = model_mnl.estimate(algoParameters={'proportionAnalyticalHessian': 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Std err</th>\n",
       "      <th>t-test</th>\n",
       "      <th>p-value</th>\n",
       "      <th>Rob. Std err</th>\n",
       "      <th>Rob. t-test</th>\n",
       "      <th>Rob. p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b_dist</th>\n",
       "      <td>-0.081780</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>-11.594702</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>-6.433521</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_edu1</th>\n",
       "      <td>6.478513</td>\n",
       "      <td>1011.239417</td>\n",
       "      <td>0.006407</td>\n",
       "      <td>0.994888</td>\n",
       "      <td>12.298068</td>\n",
       "      <td>0.526791</td>\n",
       "      <td>0.598339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_edu2</th>\n",
       "      <td>16.738849</td>\n",
       "      <td>956.941756</td>\n",
       "      <td>0.017492</td>\n",
       "      <td>0.986044</td>\n",
       "      <td>3.179523</td>\n",
       "      <td>5.264579</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_gc</th>\n",
       "      <td>-0.206337</td>\n",
       "      <td>0.089983</td>\n",
       "      <td>-2.293077</td>\n",
       "      <td>0.021844</td>\n",
       "      <td>0.10521</td>\n",
       "      <td>-1.961192</td>\n",
       "      <td>0.049857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_pop</th>\n",
       "      <td>0.604470</td>\n",
       "      <td>0.205196</td>\n",
       "      <td>2.945812</td>\n",
       "      <td>0.003221</td>\n",
       "      <td>0.200403</td>\n",
       "      <td>3.016277</td>\n",
       "      <td>0.002559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_size</th>\n",
       "      <td>0.713185</td>\n",
       "      <td>0.159512</td>\n",
       "      <td>4.471051</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.145574</td>\n",
       "      <td>4.899139</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of estimated parameters</th>\n",
       "      <td>6.000000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample size</th>\n",
       "      <td>100.000000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Excluded observations</th>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Init log likelihood</th>\n",
       "      <td>-236.549054</td>\n",
       "      <td>.7g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final log likelihood</th>\n",
       "      <td>-234.497861</td>\n",
       "      <td>.7g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Likelihood ratio test for the init. model</th>\n",
       "      <td>4.102387</td>\n",
       "      <td>.7g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho-square for the init. model</th>\n",
       "      <td>0.008671</td>\n",
       "      <td>.3g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho-square-bar for the init. model</th>\n",
       "      <td>-0.016693</td>\n",
       "      <td>.3g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Akaike Information Criterion</th>\n",
       "      <td>480.995722</td>\n",
       "      <td>.7g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian Information Criterion</th>\n",
       "      <td>496.626743</td>\n",
       "      <td>.7g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final gradient norm</th>\n",
       "      <td>0.000211</td>\n",
       "      <td>.4E</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nbr of threads</th>\n",
       "      <td>64.000000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Value      Std err     t-test  \\\n",
       "b_dist                                      -0.081780     0.007053 -11.594702   \n",
       "b_edu1                                       6.478513  1011.239417   0.006407   \n",
       "b_edu2                                      16.738849   956.941756   0.017492   \n",
       "b_gc                                        -0.206337     0.089983  -2.293077   \n",
       "b_pop                                        0.604470     0.205196   2.945812   \n",
       "b_size                                       0.713185     0.159512   4.471051   \n",
       "Number of estimated parameters               6.000000                           \n",
       "Sample size                                100.000000                           \n",
       "Excluded observations                        0.000000                           \n",
       "Init log likelihood                       -236.549054          .7g              \n",
       "Final log likelihood                      -234.497861          .7g              \n",
       "Likelihood ratio test for the init. model    4.102387          .7g              \n",
       "Rho-square for the init. model               0.008671          .3g              \n",
       "Rho-square-bar for the init. model          -0.016693          .3g              \n",
       "Akaike Information Criterion               480.995722          .7g              \n",
       "Bayesian Information Criterion             496.626743          .7g              \n",
       "Final gradient norm                          0.000211          .4E              \n",
       "Nbr of threads                              64.000000                           \n",
       "\n",
       "                                            p-value Rob. Std err Rob. t-test  \\\n",
       "b_dist                                          0.0     0.012712   -6.433521   \n",
       "b_edu1                                     0.994888    12.298068    0.526791   \n",
       "b_edu2                                     0.986044     3.179523    5.264579   \n",
       "b_gc                                       0.021844      0.10521   -1.961192   \n",
       "b_pop                                      0.003221     0.200403    3.016277   \n",
       "b_size                                     0.000008     0.145574    4.899139   \n",
       "Number of estimated parameters                                                 \n",
       "Sample size                                                                    \n",
       "Excluded observations                                                          \n",
       "Init log likelihood                                                            \n",
       "Final log likelihood                                                           \n",
       "Likelihood ratio test for the init. model                                      \n",
       "Rho-square for the init. model                                                 \n",
       "Rho-square-bar for the init. model                                             \n",
       "Akaike Information Criterion                                                   \n",
       "Bayesian Information Criterion                                                 \n",
       "Final gradient norm                                                            \n",
       "Nbr of threads                                                                 \n",
       "\n",
       "                                          Rob. p-value  \n",
       "b_dist                                             0.0  \n",
       "b_edu1                                        0.598339  \n",
       "b_edu2                                             0.0  \n",
       "b_gc                                          0.049857  \n",
       "b_pop                                         0.002559  \n",
       "b_size                                        0.000001  \n",
       "Number of estimated parameters                          \n",
       "Sample size                                             \n",
       "Excluded observations                                   \n",
       "Init log likelihood                                     \n",
       "Final log likelihood                                    \n",
       "Likelihood ratio test for the init. model               \n",
       "Rho-square for the init. model                          \n",
       "Rho-square-bar for the init. model                      \n",
       "Akaike Information Criterion                            \n",
       "Bayesian Information Criterion                          \n",
       "Final gradient norm                                     \n",
       "Nbr of threads                                          "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = estimated_model.getEstimatedParameters()\n",
    "for key, val in estimated_model.getGeneralStatistics().items():\n",
    "    result.loc[key] = [val[0], val[1]] + ['' for i in range(len(result.columns)-2)]\n",
    "result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write results to an Excel file\n",
    "writer = pd.ExcelWriter(input_path + 'estimation_results_dest_purpose_car_dist_mnl_sumOfAllAttrInLog_urban.xls', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#results.to_excel(writer, sheet_name=model_mnl.modelName)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Run estimation for distance class-segmented destination choice\n",
    "results = []\n",
    "for p in [1,2,3,4,6,7]:\n",
    "    for car in [0,1]:\n",
    "        skip_next = False\n",
    "        for i in range(len(bins)-1):\n",
    "            if skip_next:\n",
    "                continue\n",
    "            # Filter the data\n",
    "            mask = (df['PURPOSE']==p) & (df['CAR_AV']==car)\n",
    "            database = db.Database('MiD2017', df.loc[mask].copy())\n",
    "            database.remove(DIST_bin<=bins[i])\n",
    "            # Check if the next distance classes had enough observations\n",
    "            if len(database.data.loc[df['DIST_bin']>bins[i+1]]) < 50:\n",
    "                skip_next = True\n",
    "            else:\n",
    "                database.remove(DIST_bin>bins[i+1])\n",
    "            print('Sample size for purpose {}, car av. {}, distance {}: {}'.format(\n",
    "                p, car, bins[i+1], database.getSampleSize()))\n",
    "            mnl = models.loglogit(V_D[p][car], None, D)\n",
    "            formulas = {'loglike': mnl, 'weight': W_GEW}\n",
    "            model = bio.BIOGEME(database, formulas)\n",
    "            model.modelName = str(p) + '_' + str(car) + '_' + str(bins[i+1]) # Name it\n",
    "            results.append(model.estimate()) # Estimation\n",
    "            output = results[-1].getEstimatedParameters()\n",
    "            # Add results to the Excel file\n",
    "            for key, val in results[-1].getGeneralStatistics().items():\n",
    "                output.loc[key] = [val[0], val[1]] + ['' for i in range(len(output.columns)-2)]\n",
    "            output.to_excel(writer, sheet_name=model.modelName)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to an Excel file\n",
    "writer = pd.ExcelWriter(input_path + 'estimation_results_dest_bounds_inter.xls', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for purpose 1, car av. 0: 356\n",
      "[10:12:09] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 1, car av. 1: 2000\n",
      "Sample size for purpose 2, car av. 0: 91\n",
      "Sample size for purpose 2, car av. 1: 2000\n",
      "Sample size for purpose 3, car av. 0: 55\n",
      "[10:14:40] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 3, car av. 1: 2000\n",
      "Sample size for purpose 4, car av. 0: 351\n",
      "[10:16:51] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 4, car av. 1: 2000\n",
      "[10:17:47] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 6, car av. 0: 605\n",
      "[10:19:39] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 6, car av. 1: 2000\n",
      "[10:20:57] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 7, car av. 0: 41\n",
      "[10:27:47] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 7, car av. 1: 2000\n"
     ]
    }
   ],
   "source": [
    "# Run the estimation by purpose\n",
    "# and by car ownership\n",
    "results = []\n",
    "for p in [1,2,3,4,6,7]:\n",
    "    for car in [0,1]:\n",
    "        mask = (df['PURPOSE']==p) & (df['CAR_AV']==car)\n",
    "        num_inter = len(df.loc[(df['D']!=zone_dict['STAY']) & (df['O']!=df['D']) & mask])\n",
    "        num_inter = min(num_inter, 2000)\n",
    "        database = db.Database('MiD2017', pd.concat([\n",
    "            df.loc[(df['D']!=zone_dict['STAY']) & (df['O']!=df['D']) & mask].sample(num_inter) # inter-zonal trips\n",
    "            #df.loc[(df['D']!=zone_dict['STAY']) & (df['O']==df['D']) & mask].sample(num_inter) # inner-zonal trips\n",
    "            #df.loc[(df['D']==zone_dict['STAY']) & mask].sample(num_inter) # stay observations\n",
    "        ]))\n",
    "        print('Sample size for purpose {}, car av. {}: {}'.format(\n",
    "            p, car, database.getSampleSize()))\n",
    "        #nl = models.lognested(V_D[p][car], None, urban_nests, D)\n",
    "        mnl = models.loglogit(V_D[p][car], None, D)\n",
    "        formulas = {'loglike': mnl, 'weight': W_GEW}\n",
    "        model = bio.BIOGEME(database, formulas)\n",
    "        model.modelName = str(p) + '_' + str(car) # Name it\n",
    "        results.append(model.estimate(algoParameters={'proportionAnalyticalHessian': 0.5})\n",
    "                      ) # Estimation\n",
    "        output = results[-1].getEstimatedParameters()\n",
    "        # Add results to the Excel file\n",
    "        for key, val in results[-1].getGeneralStatistics().items():\n",
    "            output.loc[key] = [val[0], val[1]] + ['' for i in range(len(output.columns)-2)]\n",
    "        output.to_excel(writer, sheet_name=model.modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2154.2,\n",
       " -5270.4,\n",
       " -589.7,\n",
       " -6976.6,\n",
       " -497.8,\n",
       " -4799.6,\n",
       " -2280.3,\n",
       " -4762.6,\n",
       " -4304.6,\n",
       " -7255.0,\n",
       " -157.6,\n",
       " -5649.0]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final log likelihoods\n",
    "'''\n",
    "using simple logs of attraction variables plus log(PopDens) and the CC\n",
    "or the Daly (1982) version with sum of attraction variables in the log\n",
    "                      PopDens  Prio1  Prio2  Prio3  All3   Daly3  DalyDist   DalyCCSq DalyDist2 DalyDistUrb DalyDistLogDist\n",
    "commuting, car av. 0: -2552.4, -2467  -2467  -2467  -2467  -2467  -2138      -2308    -2131     -2129       -1946\n",
    "commuting, car av. 1: -6284.0, -6075  -6622  -6920  -6449  -6100  -4669      -6823    -5125     -4797       -5446\n",
    "business , car av. 0: -690.8,  -653   -653   -653   -653   -653   -590       -606     -4736     -589        -589\n",
    "business , car av. 1: -8732.7, -8230  -8001  -8309  -8182  -8078  -7463      -8244    -9827     -6849       -6998\n",
    "education, car av. 0: -627.4,  -627   -627   -627   -621   -627   -483       -435     -482      -480        -432\n",
    "education, car av. 1: -8004.0, -7583  -7678  -7701  -7661  -7539  -4757      -7561    -63078    -4852       -4596\n",
    "buy/exec., car av. 0: -2797.0, -2743  -2768  -2736  -2730  -2736  -2278      -2729    -2115     -2277       -2260\n",
    "buy/exec., car av. 1: -6605.9, -6671  -7123  -6899  -6446  -7056  -4683      -6253    -4836     -4716       -5001\n",
    "leisure  , car av. 0: -4656.5, -4552  -4576  -4561  -4546  -4547  -4304      -4545    -4089     -4303       -4303\n",
    "leisure  , car av. 1: -9258.1, -8627  -8720  -8552  -8498  -9368  -7317      -8734    -6219     -6828       -6503\n",
    "accompany, car av. 0: -176.9,  -167   -171   -169   -166   -169   -157       -161     -157      -157        -156\n",
    "accompany, car av. 1: -8613.2  -8484  -8178  -8442  -8461  -8747  -5593      -8309    -5038     -6072       -5247\n",
    "correct beta signs  : false    true   true   true   notEdu true   noCarFalse false    false     false       false\n",
    "suffic. significant : true     true   true   true   true   true   true       false    false     false       true\n",
    " '''\n",
    "[np.round(r.getGeneralStatistics()['Final log likelihood'][0],1) for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for purpose 1, car av. 0: 712\n",
      "Sample size for purpose 1, car av. 1: 4000\n",
      "Sample size for purpose 2, car av. 0: 182\n",
      "Sample size for purpose 2, car av. 1: 4000\n",
      "Sample size for purpose 3, car av. 0: 110\n",
      "Sample size for purpose 3, car av. 1: 4000\n",
      "Sample size for purpose 4, car av. 0: 702\n",
      "Sample size for purpose 4, car av. 1: 4000\n",
      "Sample size for purpose 6, car av. 0: 1210\n",
      "Sample size for purpose 6, car av. 1: 4000\n",
      "Sample size for purpose 7, car av. 0: 82\n",
      "Sample size for purpose 7, car av. 1: 4000\n"
     ]
    }
   ],
   "source": [
    "# Run the estimation for stay/go choice\n",
    "writer = pd.ExcelWriter(input_path + 'estimation_results_go.xls', engine='xlsxwriter')\n",
    "results_go = []\n",
    "for p in [1,2,3,4,6,7]:\n",
    "    for car in [0,1]:\n",
    "        mask = (df['PURPOSE']==p) & (df['CAR_AV']==car)\n",
    "        num_inter = len(df.loc[(df['D']!=zone_dict['STAY']) & (df['O']!=df['D']) & mask])\n",
    "        num_inter = min(num_inter, 2000)\n",
    "        database = db.Database('MiD2017', pd.concat([\n",
    "            df.loc[(df['D']!=zone_dict['STAY']) & (df['O']!=df['D']) & mask].sample(num_inter), # inter-zonal trips\n",
    "            df.loc[(df['D']!=zone_dict['STAY']) & (df['O']==df['D']) & mask].sample(num_inter) # inner-zonal trips\n",
    "            #df.loc[(df['D']==zone_dict['STAY']) & mask].sample(num_inter) # stay observations\n",
    "        ]))\n",
    "        print('Sample size for purpose {}, car av. {}: {}'.format(\n",
    "            p, car, database.getSampleSize()))\n",
    "        #nl = models.lognested(V_GO[p][car], None, go_nest, GO)\n",
    "        mnl = models.loglogit(V_GO[p][car], None, GO)\n",
    "        formulas = {'loglike': mnl, 'weight': W_GEW}\n",
    "        model = bio.BIOGEME(database, formulas)\n",
    "        model.modelName = str(p) + '_' + str(car) # Name it\n",
    "        results_go.append(model.estimate()) # Estimation\n",
    "        output = results_go[-1].getEstimatedParameters()\n",
    "        # Add results to the Excel file\n",
    "        for key, val in results_go[-1].getGeneralStatistics().items():\n",
    "            output.loc[key] = [val[0], val[1]] + ['' for i in range(len(output.columns)-2)]\n",
    "        output.to_excel(writer, sheet_name=model.modelName)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
