{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # for automation and parallelisation\n",
    "manual, scenario = (True, 'base') if 'ipykernel' in sys.argv[0] else (False, sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import biogeme.database as db\n",
    "import biogeme.biogeme as bio\n",
    "import biogeme.models as models\n",
    "import biogeme.optimization as opt\n",
    "import biogeme.messaging as message\n",
    "from biogeme import expressions as ex\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlsxwriter\n",
    "from tqdm import tqdm\n",
    "from quetzal.io import excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration\n",
    "## Estimation of the model parameters\n",
    "quetzal_germany is being estimated using [PandasBiogeme](https://biogeme.epfl.ch/). This notebook estimates calibration parameters for the model's utility functions.\n",
    "- Documentation and reference: [Bierlaire, M. (2020). A short introduction to PandasBiogeme. Technical report TRANSP-OR 200605. Transport and Mobility Laboratory, ENAC, EPFL.](https://transp-or.epfl.ch/documents/technicalReports/Bier20.pdf)\n",
    "- Tutorial: https://www.youtube.com/watch?v=OiM94B8WayA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model formulation\n",
    "\n",
    "The joint generation and distribution model makes distance classes in destination choice obsolete. It accounts for each possible destination zone plus a stay option (not making any trip).\n",
    "\n",
    "$V_i^{zone} = V_{i,m}^{mode} + log(\\sum_{j\\in A} \\beta_j a_{i,j}) + \\beta_{TOUR} a_{i,TOUR}$\n",
    "\n",
    "with $\\beta_1=1$. Zone-specific attraction attributes are covered in theory in Daly (1982; DOI: 10.1016/0191-2615(82)90037-6):\n",
    "\n",
    "$A = \\text{\\{D_POP, D_EMPL, D_SHOP, D_EDU, D_LEISURE, D_TOURISM\\}}$\n",
    "\n",
    "In the choice tree, each zone has a nest of modes to reach it. In case of sequential estimation, results of the mode tier become aggregated with the formula of generalised cost. The mode choice model consists of systematic utility functions, one for each mode in the choice set. They comprise an alternaive-specific constant (ASC), a distance-dependent part with travel time and cost summarised as generalised cost (GC), and a cost damping function F\n",
    "\n",
    "$V_{i,m}^{mode} = ASC_{i,m} + F(GC(T_m, C_m)_i, b_{gc_i})$\n",
    "\n",
    "Index i marks the demand group. I = {'commuting' (1), 'education' (2), 'shopping/medical' (3), 'business' (4), 'private' (6)} x {'no_car' (0), 'car_owned' (1)}\n",
    "\n",
    "Note: The cost variable already includes subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../input/'\n",
    "model_path = '../model/'\n",
    "output_path = '../output/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters for settings\n",
    "params = excel.read_var(file='../input/parameters.xls', scenario=scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_path + 'transport_demand/calibration_all_trips_MiD2017.csv')\n",
    "df = df[['cost_rail_short', 'cost_rail_long', 'cost_car', 'cost_coach', 'cost_bus', 'cost_walk', 'cost_air',\n",
    "         'time_rail_short', 'time_rail_long', 'time_car', 'time_coach', 'time_bus', 'time_walk', 'time_air',\n",
    "         'cost_rail', 'cost_road', 'time_rail', 'time_road',\n",
    "         'mode_model', 'purpose_model', 'purpose2', 'car_avail', 'distance', 'origin', 'destination', 'P_ID', 'W_GEW',\n",
    "         'dest_urban', 'dest_pop', 'dest_employment', 'dest_tourism', 'dest_area',\n",
    "         'dest_shopping_many', 'dest_shopping_few', 'dest_education_many', 'dest_education_few',\n",
    "         'dest_leisure_many', 'dest_leisure_few', 'dest_accompany']]\n",
    "df.columns = ['C_RAIL_S', 'C_RAIL_L', 'C_CAR', 'C_COACH', 'C_BUS', 'C_NON_MOTOR', 'C_AIR',\n",
    "              'T_RAIL_S', 'T_RAIL_L', 'T_CAR', 'T_COACH', 'T_BUS', 'T_NON_MOTOR', 'T_AIR',\n",
    "              'C_RAIL', 'C_ROAD', 'T_RAIL', 'T_ROAD',\n",
    "              'MODE', 'PURPOSE', 'P2', 'CAR_AV', 'DIST', 'O', 'D', 'P_ID', 'W_GEW',\n",
    "              'D_URBAN', 'D_POP', 'D_EMPL', 'D_TOURISM', 'D_AREA',\n",
    "              'D_SHOP-MANY', 'D_SHOP-FEW', 'D_EDU-MANY', 'D_EDU-FEW',\n",
    "              'D_LEISURE-MANY', 'D_LEISURE-FEW', 'D_ACCOM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155235"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop return trips\n",
    "df = df.loc[~df['P2'].isin([8,9])]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The estimation requires numerical purpose values\n",
    "p_model_dict = {'commuting':1, 'business':2, 'education':3,\n",
    "                'buy/execute':4, 'leisure':6, 'accompany':7}\n",
    "df['PURPOSE'] = df['PURPOSE'].apply(lambda s: p_model_dict[s.split('_')[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "inf = 10000\n",
    "df = df.replace({np.inf:inf})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3631540267401566"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scale population and employment\n",
    "df['D_POP_S'] = df['D_POP'] / 1e6\n",
    "df['D_EMPL_S'] = df['D_EMPL'] / 1e6\n",
    "df['D_POP_S'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale POIs to the same level as population\n",
    "df['D_SHOP-FEW_S'] = df['D_SHOP-FEW'] / 100\n",
    "df['D_EDU-FEW_S'] = df['D_EDU-FEW'] / 100\n",
    "df['D_LEISURE-FEW_S'] = df['D_LEISURE-FEW'] / 100\n",
    "df['D_SHOP-MANY_S'] = df['D_SHOP-MANY'] / 1000\n",
    "df['D_EDU-MANY_S'] = df['D_EDU-MANY'] / 1000\n",
    "df['D_LEISURE-MANY_S'] = df['D_LEISURE-MANY'] / 1000\n",
    "df['D_ACCOM_S'] = df['D_ACCOM'] / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make car availability binary\n",
    "df['CAR_AV'] = df['CAR_AV'].replace({9:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add PT availabilities\n",
    "df['RAIL_AV'] = (df['T_RAIL']!=inf).astype(int)\n",
    "df['RAIL_S_AV'] = (df['T_RAIL_S']!=inf).astype(int)\n",
    "df['RAIL_L_AV'] = (df['T_RAIL_L']!=inf).astype(int)\n",
    "df['COACH_AV'] = (df['T_COACH']!=inf).astype(int)\n",
    "df['BUS_AV'] = (df['T_BUS']!=inf).astype(int)\n",
    "df['ROAD_AV'] = (df['T_ROAD']!=inf).astype(int)\n",
    "df['AIR_AV'] = (df['T_AIR']!=inf).astype(int)\n",
    "df['NON_MOTOR_AV'] = (df['DIST']<100).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MODE'] = df['MODE'].replace({2:1, 3:4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hash origin and destination columns (integer)\n",
    "zone_set = set(df['D']).union(set(df['O']))\n",
    "zone_dict = dict(zip(zone_set, range(len(zone_set))))\n",
    "df['O'] = df['O'].map(zone_dict).astype(int)\n",
    "df['D'] = df['D'].map(zone_dict).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance classes\n",
    "\n",
    "Destination choice does not work for a large set of choice options, such as 400 zones. Thus, the individual choice sets are limited to various distance classes, which are consistent with the stay/go and distance choice model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155235"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean distances\n",
    "df = df.loc[df['DIST']<=1000]\n",
    "df['DIST'] = df['DIST'].astype(int)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove STAY rows for estimation without gneration\n",
    "if 'STAY' in zone_dict.keys():\n",
    "    df = df.loc[df['D']!=zone_dict['STAY']]\n",
    "    print(len(df))\n",
    "else:\n",
    "    zone_dict['STAY'] = len(zone_set)+1\n",
    "    stay_mask = df['MODE']!=999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify into distance bins\n",
    "bins = [-1, 60, 150, 1000]\n",
    "df['DIST_bin'] = pd.cut(df['DIST'], bins=bins, labels=bins[1:]).astype(int)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Origin-destination distances in MiD are very random and do not\n",
    "# neccessarily represent reality. We can obtain the most reported\n",
    "# distance class as most probable distance class for every OD pair.\n",
    "# Save distance classes into a dict by OD pair\n",
    "dist_dict = df.groupby(['O', 'D'])['DIST_bin'].value_counts().unstack().idxmax(axis=1).to_dict()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Now, we can drop observations with wrong distances or OD pairs\n",
    "# which also affected LoS attributes in the previous step\n",
    "df['DIST_bin'] = [dist_dict[a] for a in tuple(zip(df['O'], df['D']))]\n",
    "df = df.loc[df['DIST_bin']==pd.cut(df['DIST'], bins=bins, labels=bins[1:])]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Add availability columns (based on distance classes)\n",
    "for zone in tqdm(df['D'].unique()):\n",
    "    df['AV_'+str(zone)] = [dist_dict[(o,zone)] == dist_bin\n",
    "                           if (o,zone) in dist_dict.keys() else False\n",
    "                           for o,dist_bin in zip(df['O'], df['DIST_bin'])]\n",
    "    df['AV_'+str(zone)] = df['AV_'+str(zone)].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DIST_bin\n",
       "1       101174\n",
       "60       47749\n",
       "150       3966\n",
       "1000      2346\n",
       "Name: O, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a bin for STAY observations and inner-zonal trips\n",
    "df.loc[df['D']==zone_dict['STAY'], 'DIST_bin'] = 0\n",
    "df.loc[df['D']==df['O'], 'DIST_bin'] = 1\n",
    "df.groupby('DIST_bin').count()['O']#.plot.pie()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trip is inner- or inter-zonal?\n",
    "df['GO'] = df['DIST_bin'].clip(lower=0, upper=2).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model variables\n",
    "All columns are variables. DefineVariable creates a new column in the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost damping\n",
    "\n",
    "Many modelling studies have shown that cost damping is required in order to flatten the tail of time/cost elasticities, i.e. decrease the impact of long distances on choice results to prevent from overestimation of time/cost parameters. Cost damping represents the property of decreasing marginal utility. It is commonly approached with Box-Cox transformations of generalised cost (usually defined as the sum of travel time and travel expenditures divided by the value of time). Daly (2010) proposes a hybrid function as a sum of the linear term and the narural logarithm of the same. However, the linear term still dominates cost on long distances. Rich (2015), main developer of the Danish National Transport Model, proposes a more complex spline function which successfully manages cost damping and even outperforms the Box-Cox transformation in terms of stability of elasticities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The cost damping function from Rich (2020)\n",
    "c = params['estimation']\n",
    "def spline(x, beta, c1, c2, Q=3):\n",
    "    alpha = [0, -beta/2*ex.Power(ex.log(c1),3),\n",
    "             -beta/2*ex.log(c1)*(3*ex.Power(ex.log(c2),2)+ex.Power(ex.log(c1),2))] # for Q=3\n",
    "    theta = [1, 3/2*ex.log(c1), 3*ex.log(c1)*ex.log(c2)] # for Q=3\n",
    "    return (beta*theta[0]*ex.Power(ex.log(x),Q-1+1) + alpha[0]) * (x<c1) \\\n",
    "    + (beta*theta[1]*ex.Power(ex.log(x),Q-2+1) + alpha[1]) * (x>=c1)*(x<c2) \\\n",
    "    + (beta*theta[2]*ex.Power(ex.log(x),Q-3+1) + alpha[2]) * (x>=c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalised cost\n",
    "\n",
    "Both, travel time and monetary cost should be affected by cost damping measures. It is logical to define a generalised cost term `GC` with dimension of time units. This requires definition or estimation of values of time, in order to rescale monetary units, for all segments. Usually, the value of time (VoT) is distance-dependent. In the case of Germany, VoT can be taken from research undertaken within the Federal Government's transport study \"Bundesverkehrswegeplan 2030\": Axhausen et al. 2015. Ermittlung von Bewertungsansätzen für Reisezeiten und Zuverlässigkeit auf der Basis eines Modells für modale Verlagerungen im nicht-gewerblichen und gewerblichen Personenverkehr für die Bundesverkehrswegeplanung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load VoT for GC calculation\n",
    "vot = pd.read_csv(input_path + 'vot.csv', header=[0,1], index_col=0).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation of purposes (MiD2017 to VP2030)\n",
    "p_dict = {1:'Fz1', 3:'Fz2', 4:'Fz3', 2:'Fz4', 6:'Fz6', 7:'Fz6'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the GC function\n",
    "car_vot_descr = ['PT', 'all'] # for indexing with car=[0,1]\n",
    "def GC(time, price, p, car, dist):\n",
    "    return time + (price / vot[(p, car_vot_descr[car])][dist])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utility\n",
    "\n",
    "The database should have columns with prices and travel times for each mode of the trip which can be used to calculate utilities of mode choice, given the estimated beta parameters from the mode choice step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mode choice parameters\n",
    "beta_time = {} # time in minutes\n",
    "beta_price = {}\n",
    "excel = pd.ExcelFile(input_path + 'estimation_results.xls')\n",
    "p_dict_model = {1:'commuting', 2:'business', 3:'education', 4:'buy/execute', 6:'leisure', 7:'accompany'}\n",
    "for p, p_str in p_dict_model.items():\n",
    "    beta_time[p] = {}\n",
    "    beta_price[p] = {}\n",
    "    for car, car_str in {1:'_car', 0:'_no_car'}.items():\n",
    "        params_est = excel.parse(p_str.replace('/', '-')+car_str, index_col=0)\n",
    "        beta_time[p][car] = params_est.loc['b_t', 'Value']\n",
    "        try:\n",
    "            beta_price[p][car] = params_est.loc['b_c', 'Value']\n",
    "        except KeyError:\n",
    "            beta_price[p][car] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get utility of travel time\n",
    "def time_ut(time, p, car):\n",
    "    # Given time in minutes and the segment (purpose, car)\n",
    "    #return spline(time, beta_time[p][car], c['c1_time_'+str(p_dict[p][-1])]*60, c['c2_time_'+str(p_dict[p][-1])]*60)\n",
    "    # The fist part of the spline is a log-power\n",
    "    return np.power(np.log(time), 3) * beta_time[p][car]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get utility of price\n",
    "def price_ut(price, p, car):\n",
    "    # Given the price and the segment (purpose, car)\n",
    "    return price * beta_price[p][car]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Composite cost / logsum\n",
    "In a sequential estimation process, it is required to aggregate all variables of the upper decision level and insert them in this decision level. Using the formula from Walker (1977) - see formula 6.10 in Ortúzar and Willumsen (2011) p. 213 - we can assume that the perception of alternatives is depicted properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load inter-zonal composite cost from mode choice step\n",
    "cc = pd.read_csv(output_path + scenario + '/mode_choice_od_composite_cost.csv')\n",
    "# Map to integer zone names and drop unused pairs\n",
    "cc['origin'] = cc['origin'].map(zone_dict)\n",
    "cc['destination'] = cc['destination'].map(zone_dict)\n",
    "cc = cc.loc[(cc['origin'].notna()) & (cc['destination'].notna())]\n",
    "cc['origin'] = cc['origin'].astype(int)\n",
    "cc['destination'] = cc['destination'].astype(int)\n",
    "cc.set_index(['origin', 'destination'], inplace=True)\n",
    "cc.index.names = ['O', 'D']\n",
    "# Rename segments to integer values\n",
    "cc.columns = pd.MultiIndex.from_tuples(\n",
    "    [(p_model_dict[seg.split('_')[0]], {'no': 0, 'car': 1}[seg.split('_')[1]])\n",
    "     for seg in cc.columns],\n",
    "    names=['PURPOSE', 'CAR_AV'])\n",
    "# Reshape the table into a mergable format\n",
    "cc = cc.unstack('D').stack('PURPOSE').stack('CAR_AV')\n",
    "cc.columns = ['CC_'+str(zone) for zone in cc.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inter-zonal CC\n",
    "df = df.merge(cc, how='left', left_on=['O', 'PURPOSE', 'CAR_AV'], right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1933/1933 [00:46<00:00, 41.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# Inner-zonal\n",
    "for d in tqdm(list(zone_dict.values())):\n",
    "    mask = df['O']!=d\n",
    "    car_av_s = df.loc[~mask, 'CAR_AV']\n",
    "    p_s = df.loc[~mask, 'PURPOSE']\n",
    "    t_rail_s = pd.Series([time_ut(t, p, car) if t!=inf else np.nan\n",
    "                          for t,p,car in zip(df.loc[~mask, 'T_RAIL'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    t_bus_s = pd.Series([time_ut(t, p, car) if t!=inf else np.nan\n",
    "                         for t,p,car in zip(df.loc[~mask, 'T_BUS'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    t_car_s = pd.Series([time_ut(t, p, car) if car==1 else np.nan\n",
    "                         for t,p,car in zip(df.loc[~mask, 'T_CAR'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    t_non_s = pd.Series([time_ut(t, p, car) if t!=inf else np.nan\n",
    "                         for t,p,car in zip(df.loc[~mask, 'T_NON_MOTOR'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    p_rail_s = pd.Series([price_ut(t, p, car) if t!=inf else np.nan\n",
    "                          for t,p,car in zip(df.loc[~mask, 'C_RAIL'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    p_bus_s = pd.Series([price_ut(t, p, car) if t!=inf else np.nan\n",
    "                         for t,p,car in zip(df.loc[~mask, 'C_BUS'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    p_car_s = pd.Series([price_ut(t, p, car) if car==1 else np.nan\n",
    "                         for t,p,car in zip(df.loc[~mask, 'C_CAR'], p_s, car_av_s)],\n",
    "                         dtype=float)\n",
    "    # Calculate the logsum\n",
    "    df.loc[~mask, 'CC_'+str(d)] = [np.log(np.maximum(4, np.sum(\n",
    "        pd.Series([np.exp(-1*(t_rail+p_rail)),\n",
    "                   np.exp(-1*(t_bus+p_bus)),\n",
    "                   np.exp(-1*(t_car+p_car)),\n",
    "                   np.exp(-1*t_non)]))))\n",
    "                                   for t_rail, t_bus, t_car, t_non, p_rail, p_bus, p_car\n",
    "                                   in zip(t_rail_s, t_bus_s, t_car_s, t_non_s,\n",
    "                                          p_rail_s, p_bus_s, p_car_s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with segment's max values\n",
    "for p in df['PURPOSE'].unique():\n",
    "    for car in df['CAR_AV'].unique():\n",
    "        mask = (df['PURPOSE']==p) & (df['CAR_AV']==car), [c for c in df.columns if c[:2]=='CC']\n",
    "        df.loc[mask] = df.loc[mask].fillna(\n",
    "            cc.xs(p, level='PURPOSE').xs(car, level='CAR_AV').max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All CC must be greater 0\n",
    "assert df[[c for c in df.columns if c.startswith('CC_')]].min().min() > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the accessibility of the origin zone\n",
    "# Use the logsum formula with CC from all other zones\n",
    "#df['ACC'] = np.log(np.sum(np.exp(df[[c for c in df.columns if c.startswith('CC_')]]), axis=1))\n",
    "df['ACC'] = df[[c for c in df.columns if c.startswith('CC_')]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['ACC'].hist(bins=100, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155221"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add distances to the attributes\n",
    "# in case CC does not suffice as distance measure\n",
    "dist = pd.read_csv(output_path + 'distances_centroids.csv') # km\n",
    "dist = dist.loc[(dist['origin'].map(zone_dict).notna())\n",
    "                & (dist['destination'].map(zone_dict).notna())]\n",
    "dist['origin'] = dist['origin'].map(zone_dict).astype(int)\n",
    "dist['destination'] = dist['destination'].map(zone_dict).astype(int)\n",
    "dist.set_index(['origin', 'destination'], inplace=True)\n",
    "dist = dist.unstack('destination').fillna(0).droplevel(0, axis=1)\n",
    "df = df.merge(dist, how='left', left_on='O', right_index=True)\n",
    "df.rename(columns={z: 'DIST_'+str(z) for z in df['D'].unique()}, inplace=True)\n",
    "df = df.loc[df.notna().all(axis=1)]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: {1: {'corr': 0.7794627162719312,\n",
       "   'intercept': 3.265452523250122,\n",
       "   'slope': 0.011613001557131047},\n",
       "  0: {'corr': 0.9235468536649263,\n",
       "   'intercept': 5.657561806685959,\n",
       "   'slope': 0.005701226399690235}},\n",
       " 6: {1: {'corr': 0.6139636537095833,\n",
       "   'intercept': 4.597298197996851,\n",
       "   'slope': 0.0030793685683175556},\n",
       "  0: {'corr': 0.5729661612474145,\n",
       "   'intercept': 5.13942318832389,\n",
       "   'slope': 0.0020149513324241156}},\n",
       " 7: {1: {'corr': 0.7441952840178291,\n",
       "   'intercept': 3.658799455742527,\n",
       "   'slope': 0.014463195082016702},\n",
       "  0: {'corr': 0.9974676141641019,\n",
       "   'intercept': 2.9146235298113474,\n",
       "   'slope': 0.026725078510779394}},\n",
       " 1: {1: {'corr': 0.8306084851469295,\n",
       "   'intercept': 5.304996045617766,\n",
       "   'slope': 0.03083701740307867},\n",
       "  0: {'corr': 0.937747044470295,\n",
       "   'intercept': 4.165893167542781,\n",
       "   'slope': 0.0062200467131824}},\n",
       " 3: {1: {'corr': 0.7303300496267939,\n",
       "   'intercept': 6.096832132861296,\n",
       "   'slope': 0.007339966675576075},\n",
       "  0: {'corr': 0.9832840955475266,\n",
       "   'intercept': 6.817746073183706,\n",
       "   'slope': 0.012619421637167493}},\n",
       " 2: {1: {'corr': 0.15304496815816362,\n",
       "   'intercept': 4.6477823879894435,\n",
       "   'slope': 0.0004948050099610248},\n",
       "  0: {'corr': 0.3107342099853809,\n",
       "   'intercept': 5.640926341281051,\n",
       "   'slope': 0.001182918043521359}}}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the relation between CC and distance for every segment\n",
    "cc_dist_rel = {}\n",
    "for p in df['PURPOSE'].unique():\n",
    "    cc_dist_rel[p] = {}\n",
    "    for car in df['CAR_AV'].unique():\n",
    "        cc_dist_rel[p][car] = {}\n",
    "        # Merge CC and OD distance data\n",
    "        cc_stack = cc.xs(p, level='PURPOSE').xs(car, level='CAR_AV').rename(\n",
    "            columns={c: int(c.split('_')[1]) for c in cc.columns}).stack().dropna()\n",
    "        cc_stack.index.names = ['origin', 'destination']\n",
    "        cc_stack.name = 'CC'\n",
    "        merged = pd.DataFrame(cc_stack).merge(dist.stack().rename('dist'),\n",
    "                                              left_index=True, right_index=True, how='inner')\n",
    "        merged = merged.loc[merged['dist']>0]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        cc_dist_rel[p][car]['corr'] = merged.corr().loc['dist', 'CC']\n",
    "        coef = np.polyfit(x=merged['dist'], y=merged['CC'], deg=1)\n",
    "        cc_dist_rel[p][car]['intercept'] = coef[1]\n",
    "        cc_dist_rel[p][car]['slope'] = coef[0]\n",
    "cc_dist_rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zone attraction\n",
    "\n",
    "As for modes, every observation needs attraction attributes for all zones available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate zone-specific variables for every attribute\n",
    "for attr in ['D_URBAN', 'D_POP', 'D_EMPL', 'D_AREA'#,\n",
    "             #'D_SHOP-MANY', 'D_EDU-MANY', 'D_LEISURE-MANY',\n",
    "             #'D_SHOP-FEW', 'D_EDU-FEW', 'D_LEISURE-FEW', 'D_ACCOM'\n",
    "             #'D_SHOP-MANY_S', 'D_EDU-MANY_S', 'D_LEISURE-MANY_S',\n",
    "             #'D_SHOP-FEW_S', 'D_EDU-FEW_S', 'D_LEISURE-FEW_S', 'D_ACCOM_S'\n",
    "            ]:\n",
    "    attr_dict = df.set_index('D')[attr].to_dict()\n",
    "    for zone, value in attr_dict.items():\n",
    "        globals()[attr.split('_')[1] + '_' + str(zone)] = value\n",
    "    globals()[attr.split('_')[1] + '_' + str(zone_dict['STAY'])] = 0\n",
    "    df[attr.replace('D_', 'O_').replace('-', '_')] = df['O'].map(attr_dict).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['O_AREA'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHILDCARE\n",
      "DAILY_LEISURE\n",
      "HIGHER_EDUCATION\n",
      "HOLIDAY\n",
      "MEDICAL\n",
      "OCCASIONAL_LEISURE\n",
      "SCHOOL\n",
      "SHOP\n",
      "SPECIAL_SHOP\n"
     ]
    }
   ],
   "source": [
    "# Generate categories of OSM POIs\n",
    "cats = pd.read_excel(input_path + 'spatial_OSM_POI_list.xlsx', sheet_name='categories')\n",
    "pois = pd.read_csv(input_path + 'spatial_num_pois_raw.csv', index_col='index')\n",
    "cats['label'] = (cats['key'] + ' ' + cats['value'].fillna('')).str.strip()\n",
    "col_dict = cats.loc[cats['category'].notna()\n",
    "                   ].groupby('category').agg({'label': list}).to_dict()['label']\n",
    "for category, columns in col_dict.items():\n",
    "    for zone, value in pois[columns].sum(axis=1).items():\n",
    "        if zone in zone_dict.keys():\n",
    "            globals()[category.upper() + '_' + str(zone_dict[zone])] = value\n",
    "    globals()[category.upper() + '_' + str(zone_dict['STAY'])] = 0\n",
    "    df['O_'+category.upper()] = df['O'].map(attr_dict).fillna(0)\n",
    "    print(category.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create database\n",
    "\n",
    "This makes all columns become global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155221"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the initial database and make columns global variables\n",
    "database = db.Database('MiD2017', df.copy())\n",
    "globals().update(database.variables)\n",
    "database.getSampleSize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination choice / attraction\n",
    "b_urban = ex.Beta('b_urban', 0, None, None, 0)\n",
    "b_pop = ex.Beta('b_pop', 0, None, None, 0)\n",
    "b_empl = ex.Beta('b_empl', 0.1, 0, None, 0)\n",
    "b_shop1 = ex.Beta('b_shop1', 0.1, 0, None, 0)\n",
    "b_edu1 = ex.Beta('b_edu1', 0.1, 0, None, 0)\n",
    "b_leisure1 = ex.Beta('b_leisure1', 0.1, 0, None, 0)\n",
    "b_accom1 = ex.Beta('b_accom1', 0.1, 0, None, 0)\n",
    "b_shop2 = ex.Beta('b_shop2', 0.1, 0, None, 0)\n",
    "b_edu2 = ex.Beta('b_edu2', 0.1, 0, None, 0)\n",
    "b_leisure2 = ex.Beta('b_leisure2', 0.1, 0, None, 0)\n",
    "b_accom2 = ex.Beta('b_accom2', 0.1, 0, None, 0)\n",
    "b_shop3 = ex.Beta('b_shop3', 0.1, 0, None, 0)\n",
    "b_edu3 = ex.Beta('b_edu3', 0.1, 0, None, 0)\n",
    "b_leisure3 = ex.Beta('b_leisure3', 0.1, 0, None, 0)\n",
    "b_accom3 = ex.Beta('b_accom3', 0.1, 0, None, 0)\n",
    "b_local = ex.Beta('b_local', 0, None, None, 0)\n",
    "b_area = ex.Beta('b_area', 0, None, None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta parameter for size variables, following Daly 1982\n",
    "b_size = ex.Beta('b_size', 0, 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generalised cost function / mode choice utility parameter\n",
    "b_gc = ex.Beta('b_gc', -0.1, None, 0, 0)\n",
    "# Distance parameter\n",
    "b_dist = ex.Beta('b_dist', -0.1, None, 0, 0)\n",
    "b_dist2 = ex.Beta('b_dist2', -0.1, None, None, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logsum parameters\n",
    "phi = ex.Beta('phi', 0.5, 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for nests\n",
    "mu_go = ex.Beta('mu_go', 1, 1, 10, 0)\n",
    "mu_u1 = ex.Beta('mu_u1', 1, 1, 10, 0)\n",
    "mu_u2 = ex.Beta('mu_u2', 1, 1, 10, 0)\n",
    "mu_u3 = ex.Beta('mu_u3', 1, 1, 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASC for stay/go choice\n",
    "asc_stay = ex.Beta('asc_stay', 0, None, None, 0)\n",
    "asc_inner = ex.Beta('asc_inner', 0, None, None, 0)\n",
    "asc_inter = ex.Beta('asc_inter', 0, None, None, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Destination choice utility by segment\n",
    "# using the sum of logarithms of variables of size\n",
    "# sumOfLogs\n",
    "V_D = {p: {} for p in df['PURPOSE'].unique()}\n",
    "for car in df['CAR_AV'].unique():\n",
    "    # Commuting\n",
    "    V_D[1][car] = {zone: ex.log(globals()['EMPL_'+str(zone)]) * b_empl\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}\n",
    "    ## Business\n",
    "    V_D[2][car] = {zone: ex.log(globals()['EMPL_'+str(zone)]) * b_empl\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}\n",
    "    # Education\n",
    "    V_D[3][car] = {zone: ex.log(1 + globals()['HIGHER_EDUCATION_'+str(zone)]) * b_edu1\n",
    "           + ex.log(1 + globals()['SCHOOL_'+str(zone)]) * b_edu2\n",
    "           + ex.log(1 + globals()['CHILDCARE_'+str(zone)]) * b_edu3\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}\n",
    "    # Shopping\n",
    "    V_D[4][car] = {zone: ex.log(1 + globals()['MEDICAL_'+str(zone)]) * b_shop1\n",
    "           + ex.log(1 + globals()['SHOP_'+str(zone)]) * b_shop2\n",
    "           + ex.log(1 + globals()['SPECIAL_SHOP_'+str(zone)]) * b_shop3\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}\n",
    "    # Leisure\n",
    "    V_D[6][car] = {zone: ex.log(1 + globals()['HOLIDAY_'+str(zone)]) * b_leisure1\n",
    "           + ex.log(1 + globals()['DAILY_LEISURE_'+str(zone)]) * b_leisure2\n",
    "           + ex.log(1 + globals()['OCCASIONAL_LEISURE_'+str(zone)]) * b_leisure3\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}\n",
    "    # Accompany\n",
    "    V_D[7][car] = {zone: ex.log(1 + globals()['MEDICAL_'+str(zone)]) * b_accom1\n",
    "           + ex.log(1 + globals()['SCHOOL_'+str(zone)]) * b_accom2\n",
    "           + ex.log(1 + globals()['CHILDCARE_'+str(zone)]) * b_accom3\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           for zone in df['D'].unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Destination choice utility by segment\n",
    "# using the sum of variables of size in a logarithm\n",
    "# Daly3\n",
    "V_D = {}\n",
    "for p in df['PURPOSE'].unique():\n",
    "    V_D[p] = {}\n",
    "for car in df['CAR_AV'].unique():\n",
    "    # Commuting\n",
    "    V_D[1][car] = {zone: ex.log(globals()['EMPL_'+str(zone)]) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           for zone in df['D'].unique()}\n",
    "    ## Business\n",
    "    V_D[2][car] = {zone: ex.log(globals()['EMPL_'+str(zone)]) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           + globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           for zone in df['D'].unique()}\n",
    "    # Education\n",
    "    V_D[3][car] = {zone: ex.log(1 + globals()['CHILDCARE_'+str(zone)] * ex.exp(b_edu1)\n",
    "                                + globals()['SCHOOL_'+str(zone)] * ex.exp(b_edu2)\n",
    "                                + globals()['HIGHER_EDUCATION_'+str(zone)]\n",
    "                               ) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)] )/ globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           for zone in df['D'].unique()}\n",
    "    # Shopping\n",
    "    V_D[4][car] = {zone: ex.log(1 + globals()['SHOP_'+str(zone)] * ex.exp(b_shop1)\n",
    "                                + globals()['MEDICAL_'+str(zone)] * ex.exp(b_shop2)\n",
    "                                + globals()['SPECIAL_SHOP_'+str(zone)]\n",
    "                               ) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           for zone in df['D'].unique()}\n",
    "    # Leisure\n",
    "    V_D[6][car] = {zone: ex.log(1 + globals()['DAILY_LEISURE_'+str(zone)] * ex.exp(b_leisure1)\n",
    "                                + globals()['HOLIDAY_'+str(zone)] * ex.exp(b_leisure2)\n",
    "                                + globals()['OCCASIONAL_LEISURE_'+str(zone)]\n",
    "                               ) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           + globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           + globals()['DIST_'+str(zone)] * globals()['DIST_'+str(zone)] * b_dist2 * int(car>0)\n",
    "           for zone in df['D'].unique()}\n",
    "    # Accompany\n",
    "    V_D[7][car] = {zone: ex.log(1 + globals()['MEDICAL_'+str(zone)] * ex.exp(b_accom1)\n",
    "                                + globals()['SCHOOL_'+str(zone)] * ex.exp(b_accom2)\n",
    "                                + globals()['CHILDCARE_'+str(zone)]\n",
    "                               ) * b_size\n",
    "           + ex.log((1 + globals()['POP_'+str(zone)]) / globals()['AREA_'+str(zone)]) * b_pop\n",
    "           #+ globals()['URBAN_'+str(zone)] * b_urban\n",
    "           + globals()['CC_'+str(zone)] * b_gc\n",
    "           + globals()['DIST_'+str(zone)] * b_dist\n",
    "           + globals()['DIST_'+str(zone)] * globals()['DIST_'+str(zone)] * b_dist2\n",
    "           for zone in df['D'].unique()}\n",
    "\n",
    "# The utility for STAY is zero\n",
    "for p in V_D.keys():\n",
    "    for car in [0,1]:\n",
    "        V_D[p][car][zone_dict['STAY']] = 0 #asc_stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The utility for STAY/inner/inter\n",
    "V_GO = {}\n",
    "for p in df['PURPOSE'].unique():\n",
    "    V_GO[p] = {car: {} for car in df['CAR_AV'].unique()}\n",
    "    for car in df['CAR_AV'].unique():\n",
    "        V_GO[p][car][0] = 0 #asc_stay\n",
    "        V_GO[p][car][2] = asc_inter + ACC * b_gc\n",
    "\n",
    "# Utility for inner-zonal travel depends on zone POIs\n",
    "b_size = ex.Beta('b_size', 0, 0, None, 0)\n",
    "\n",
    "for car in df['CAR_AV'].unique():\n",
    "    # Commuting\n",
    "    V_GO[1][car][1] = ex.log(1 + O_EMPL) * b_size \\\n",
    "           + ex.log((1 + O_POP) / O_AREA) * b_pop + O_URBAN * b_urban\n",
    "    ## Business\n",
    "    V_GO[2][car][1] = ex.log(1 + O_EMPL) * b_size \\\n",
    "           + ex.log((1 + O_POP) / O_AREA) * b_pop + O_URBAN * b_urban\n",
    "    # Education\n",
    "    V_GO[3][car][1] = ex.log(1 + O_CHILDCARE# * ex.exp(b_edu1)\n",
    "                             + O_SCHOOL# * ex.exp(b_edu2)\n",
    "                             + O_HIGHER_EDUCATION\n",
    "                            ) * b_size \\\n",
    "           + ex.log((1 + O_POP) / O_AREA) * b_pop + O_URBAN * b_urban\n",
    "    # Shopping\n",
    "    V_GO[4][car][1] = ex.log(1 + O_SHOP# * ex.exp(b_shop1)\n",
    "                             + O_MEDICAL# * ex.exp(b_shop2)\n",
    "                             + O_SPECIAL_SHOP\n",
    "                            ) * b_size \\\n",
    "           + ex.log((1 + O_POP) / O_AREA) * b_pop + O_URBAN * b_urban\n",
    "    # Leisure\n",
    "    V_GO[6][car][1] = ex.log(1 + O_DAILY_LEISURE# * ex.exp(b_leisure1)\n",
    "                             + O_HOLIDAY# * ex.exp(b_leisure2)\n",
    "                             + O_OCCASIONAL_LEISURE\n",
    "                            ) * b_size \\\n",
    "           + ex.log(1 + O_POP / O_AREA) * b_pop + O_URBAN * b_urban\n",
    "    # Accompany\n",
    "    V_GO[7][car][1] = ex.log(1 + O_MEDICAL# * ex.exp(b_accom1)\n",
    "                             + O_SCHOOL# * ex.exp(b_accom2)\n",
    "                             + O_CHILDCARE\n",
    "                            ) * b_size \\\n",
    "           + ex.log(1 + O_POP / O_AREA) * b_pop + O_URBAN * b_urban"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The utility for STAY/inner/inter\n",
    "V_GO = {}\n",
    "for p in df['PURPOSE'].unique():\n",
    "    V_GO[p] = {car: {} for car in df['CAR_AV'].unique()}\n",
    "    for car in df['CAR_AV'].unique():\n",
    "        V_GO[p][car][0] = 0 #asc_stay\n",
    "        V_GO[p][car][2] = asc_inter + ACC * b_gc\n",
    "\n",
    "# Utility for inner-zonal travel depends on zone POIs\n",
    "b_size = ex.Beta('b_size', 0, 0, None, 0)\n",
    "\n",
    "for car in df['CAR_AV'].unique():\n",
    "    # Commuting\n",
    "    V_GO[1][car][1] = ex.log(1 + O_EMPL) * b_size \\\n",
    "           + ex.Power(O_AREA, 0.5) * b_gc * cc_dist_rel[1][car]['slope']# + O_URBAN * b_urban\n",
    "    ## Business\n",
    "    V_GO[2][car][1] = ex.log(1 + O_EMPL) * b_size \\\n",
    "           + ex.Power(O_AREA, 0.5) * b_gc * cc_dist_rel[2][car]['slope']# + O_URBAN * b_urban\n",
    "    # Education\n",
    "    V_GO[3][car][1] = ex.log(1 + O_CHILDCARE# * ex.exp(b_edu1)\n",
    "                             + O_SCHOOL# * ex.exp(b_edu2)\n",
    "                             + O_HIGHER_EDUCATION\n",
    "                            ) * b_size \\\n",
    "           + ex.Power(O_AREA, 0.5) * b_gc * cc_dist_rel[3][car]['slope']# + O_URBAN * b_urban\n",
    "    # Shopping\n",
    "    V_GO[4][car][1] = ex.log(1 + O_SHOP# * ex.exp(b_shop1)\n",
    "                             + O_MEDICAL# * ex.exp(b_shop2)\n",
    "                             + O_SPECIAL_SHOP\n",
    "                            ) * b_size \\\n",
    "           + ex.Power(O_AREA, 0.5) * b_gc * cc_dist_rel[4][car]['slope']# + O_URBAN * b_urban\n",
    "    # Leisure\n",
    "    V_GO[6][car][1] = ex.log(1 + O_DAILY_LEISURE# * ex.exp(b_leisure1)\n",
    "                             + O_HOLIDAY# * ex.exp(b_leisure2)\n",
    "                             + O_OCCASIONAL_LEISURE\n",
    "                            ) * b_size \\\n",
    "           + ex.Power(O_AREA, 0.5) * b_gc * cc_dist_rel[6][car]['slope']# + O_URBAN * b_urban\n",
    "    # Accompany\n",
    "    V_GO[7][car][1] = ex.log(1 + O_MEDICAL# * ex.exp(b_accom1)\n",
    "                             + O_SCHOOL# * ex.exp(b_accom2)\n",
    "                             + O_CHILDCARE\n",
    "                            ) * b_size \\\n",
    "           + ex.Power(O_AREA, 0.5) * b_gc * cc_dist_rel[7][car]['slope']# + O_URBAN * b_urban"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define level of verbosity\n",
    "logger = message.bioMessage()\n",
    "#logger.setSilent()\n",
    "logger.setWarning()\n",
    "#logger.setGeneral()\n",
    "#logger.setDetailed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Availability computed through OD distance\n",
    "av = {zone: (globals()['DIST_'+str(zone)] < 100)\n",
    "      for zone in df['D'].unique()}\n",
    "av[zone_dict['STAY']] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Urbanisation nests\n",
    "nest_u1 = mu_u1, list(set(df.loc[df['D_URBAN']==1, 'D']))\n",
    "nest_u2 = mu_u2, list(set(df.loc[df['D_URBAN']==2, 'D']))\n",
    "nest_u3 = mu_u3, list(set(df.loc[df['D_URBAN']==3, 'D']))\n",
    "stay = 1, [zone_dict['STAY']]\n",
    "urban_nests = nest_u1, nest_u2, nest_u3, stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stay/go nests\n",
    "stay = 1, [zone_dict['STAY']]\n",
    "go = mu_go, [d for d in zone_dict.values() if d!=zone_dict['STAY']]\n",
    "go_nest = stay, go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# TEST DATABASE\n",
    "mask = (df['PURPOSE']==3)\n",
    "database = db.Database('MiD2017', pd.concat([\n",
    "    df.loc[(df['D']!=zone_dict['STAY']) & (df['O']!=df['D']) & mask].sample(100) # inter-zonal trips\n",
    "    #df.loc[(df['D']!=zone_dict['STAY']) & (df['O']==df['D']) & mask].sample(500) # inner-zonal trips\n",
    "    #df.loc[(df['D']==zone_dict['STAY']) & mask].sample(300)]) # stay observations\n",
    "]))\n",
    "print(database.getSampleSize())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TEST estimation with Nested Logit\n",
    "nl = models.lognested(V_D[3][1], None, urban_nests, D)\n",
    "model_nl = bio.BIOGEME(database, nl)\n",
    "model_nl.modelName = 'NL'\n",
    "estimated_model = model_nl.estimate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TEST  estimation MNL\n",
    "mnl = models.loglogit(V_D[3][1], av, D)\n",
    "model_mnl = bio.BIOGEME(database, mnl)\n",
    "model_mnl.modelName = 'MNL'\n",
    "estimated_model = model_mnl.estimate(algoParameters={'proportionAnalyticalHessian': 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Active bound</th>\n",
       "      <th>Std err</th>\n",
       "      <th>t-test</th>\n",
       "      <th>p-value</th>\n",
       "      <th>Rob. Std err</th>\n",
       "      <th>Rob. t-test</th>\n",
       "      <th>Rob. p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>b_dist</th>\n",
       "      <td>-0.101355</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.009052</td>\n",
       "      <td>-11.196489</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.01008</td>\n",
       "      <td>-10.055425</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_edu1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1797693134862315708145274237317043567980705675...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.713</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_edu2</th>\n",
       "      <td>10.929838</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1797693134862315708145274237317043567980705675...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.719714</td>\n",
       "      <td>6.355615</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_gc</th>\n",
       "      <td>-0.452630</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.143584</td>\n",
       "      <td>-3.152363</td>\n",
       "      <td>0.00162</td>\n",
       "      <td>0.173507</td>\n",
       "      <td>-2.608712</td>\n",
       "      <td>0.009088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_pop</th>\n",
       "      <td>0.441958</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.215471</td>\n",
       "      <td>2.051119</td>\n",
       "      <td>0.040255</td>\n",
       "      <td>0.197284</td>\n",
       "      <td>2.240215</td>\n",
       "      <td>0.025077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b_size</th>\n",
       "      <td>0.930846</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.176454</td>\n",
       "      <td>5.275302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.16851</td>\n",
       "      <td>5.523985</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of estimated parameters</th>\n",
       "      <td>6.000000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Number of free parameters</th>\n",
       "      <td>5.000000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sample size</th>\n",
       "      <td>100.000000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Excluded observations</th>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Init log likelihood</th>\n",
       "      <td>-195.845282</td>\n",
       "      <td>.7g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final log likelihood</th>\n",
       "      <td>-195.844921</td>\n",
       "      <td>.7g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Likelihood ratio test for the init. model</th>\n",
       "      <td>0.000723</td>\n",
       "      <td>.7g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho-square for the init. model</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>.3g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rho-square-bar for the init. model</th>\n",
       "      <td>-0.030635</td>\n",
       "      <td>.3g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Akaike Information Criterion</th>\n",
       "      <td>403.689841</td>\n",
       "      <td>.7g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian Information Criterion</th>\n",
       "      <td>419.320862</td>\n",
       "      <td>.7g</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Final gradient norm</th>\n",
       "      <td>0.000277</td>\n",
       "      <td>.4E</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Nbr of threads</th>\n",
       "      <td>64.000000</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Value Active bound  \\\n",
       "b_dist                                      -0.101355          0.0   \n",
       "b_edu1                                       0.000000          1.0   \n",
       "b_edu2                                      10.929838          0.0   \n",
       "b_gc                                        -0.452630          0.0   \n",
       "b_pop                                        0.441958          0.0   \n",
       "b_size                                       0.930846          0.0   \n",
       "Number of estimated parameters               6.000000                \n",
       "Number of free parameters                    5.000000                \n",
       "Sample size                                100.000000                \n",
       "Excluded observations                        0.000000                \n",
       "Init log likelihood                       -195.845282          .7g   \n",
       "Final log likelihood                      -195.844921          .7g   \n",
       "Likelihood ratio test for the init. model    0.000723          .7g   \n",
       "Rho-square for the init. model               0.000002          .3g   \n",
       "Rho-square-bar for the init. model          -0.030635          .3g   \n",
       "Akaike Information Criterion               403.689841          .7g   \n",
       "Bayesian Information Criterion             419.320862          .7g   \n",
       "Final gradient norm                          0.000277          .4E   \n",
       "Nbr of threads                              64.000000                \n",
       "\n",
       "                                                                                     Std err  \\\n",
       "b_dist                                                                              0.009052   \n",
       "b_edu1                                     1797693134862315708145274237317043567980705675...   \n",
       "b_edu2                                     1797693134862315708145274237317043567980705675...   \n",
       "b_gc                                                                                0.143584   \n",
       "b_pop                                                                               0.215471   \n",
       "b_size                                                                              0.176454   \n",
       "Number of estimated parameters                                                                 \n",
       "Number of free parameters                                                                      \n",
       "Sample size                                                                                    \n",
       "Excluded observations                                                                          \n",
       "Init log likelihood                                                                            \n",
       "Final log likelihood                                                                           \n",
       "Likelihood ratio test for the init. model                                                      \n",
       "Rho-square for the init. model                                                                 \n",
       "Rho-square-bar for the init. model                                                             \n",
       "Akaike Information Criterion                                                                   \n",
       "Bayesian Information Criterion                                                                 \n",
       "Final gradient norm                                                                            \n",
       "Nbr of threads                                                                                 \n",
       "\n",
       "                                              t-test   p-value Rob. Std err  \\\n",
       "b_dist                                    -11.196489       0.0      0.01008   \n",
       "b_edu1                                           0.0       1.0        2.713   \n",
       "b_edu2                                           0.0       1.0     1.719714   \n",
       "b_gc                                       -3.152363   0.00162     0.173507   \n",
       "b_pop                                       2.051119  0.040255     0.197284   \n",
       "b_size                                      5.275302       0.0      0.16851   \n",
       "Number of estimated parameters                                                \n",
       "Number of free parameters                                                     \n",
       "Sample size                                                                   \n",
       "Excluded observations                                                         \n",
       "Init log likelihood                                                           \n",
       "Final log likelihood                                                          \n",
       "Likelihood ratio test for the init. model                                     \n",
       "Rho-square for the init. model                                                \n",
       "Rho-square-bar for the init. model                                            \n",
       "Akaike Information Criterion                                                  \n",
       "Bayesian Information Criterion                                                \n",
       "Final gradient norm                                                           \n",
       "Nbr of threads                                                                \n",
       "\n",
       "                                          Rob. t-test Rob. p-value  \n",
       "b_dist                                     -10.055425          0.0  \n",
       "b_edu1                                            0.0          1.0  \n",
       "b_edu2                                       6.355615          0.0  \n",
       "b_gc                                        -2.608712     0.009088  \n",
       "b_pop                                        2.240215     0.025077  \n",
       "b_size                                       5.523985          0.0  \n",
       "Number of estimated parameters                                      \n",
       "Number of free parameters                                           \n",
       "Sample size                                                         \n",
       "Excluded observations                                               \n",
       "Init log likelihood                                                 \n",
       "Final log likelihood                                                \n",
       "Likelihood ratio test for the init. model                           \n",
       "Rho-square for the init. model                                      \n",
       "Rho-square-bar for the init. model                                  \n",
       "Akaike Information Criterion                                        \n",
       "Bayesian Information Criterion                                      \n",
       "Final gradient norm                                                 \n",
       "Nbr of threads                                                      "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = estimated_model.getEstimatedParameters()\n",
    "for key, val in estimated_model.getGeneralStatistics().items():\n",
    "    result.loc[key] = [val[0], val[1]] + ['' for i in range(len(result.columns)-2)]\n",
    "result"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Write results to an Excel file\n",
    "writer = pd.ExcelWriter(input_path + 'estimation_results_dest_purpose_car_dist_mnl_sumOfAllAttrInLog_urban.xls', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#results.to_excel(writer, sheet_name=model_mnl.modelName)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Run estimation for distance class-segmented destination choice\n",
    "results = []\n",
    "for p in [1,2,3,4,6,7]:\n",
    "    for car in [0,1]:\n",
    "        skip_next = False\n",
    "        for i in range(len(bins)-1):\n",
    "            if skip_next:\n",
    "                continue\n",
    "            # Filter the data\n",
    "            mask = (df['PURPOSE']==p) & (df['CAR_AV']==car)\n",
    "            database = db.Database('MiD2017', df.loc[mask].copy())\n",
    "            database.remove(DIST_bin<=bins[i])\n",
    "            # Check if the next distance classes had enough observations\n",
    "            if len(database.data.loc[df['DIST_bin']>bins[i+1]]) < 50:\n",
    "                skip_next = True\n",
    "            else:\n",
    "                database.remove(DIST_bin>bins[i+1])\n",
    "            print('Sample size for purpose {}, car av. {}, distance {}: {}'.format(\n",
    "                p, car, bins[i+1], database.getSampleSize()))\n",
    "            mnl = models.loglogit(V_D[p][car], None, D)\n",
    "            formulas = {'loglike': mnl, 'weight': W_GEW}\n",
    "            model = bio.BIOGEME(database, formulas)\n",
    "            model.modelName = str(p) + '_' + str(car) + '_' + str(bins[i+1]) # Name it\n",
    "            results.append(model.estimate()) # Estimation\n",
    "            output = results[-1].getEstimatedParameters()\n",
    "            # Add results to the Excel file\n",
    "            for key, val in results[-1].getGeneralStatistics().items():\n",
    "                output.loc[key] = [val[0], val[1]] + ['' for i in range(len(output.columns)-2)]\n",
    "            output.to_excel(writer, sheet_name=model.modelName)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to an Excel file\n",
    "writer = pd.ExcelWriter(input_path + 'estimation_results_dest_bounds_inter.xls', engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for purpose 1, car av. 0: 356\n",
      "[10:12:09] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 1, car av. 1: 2000\n",
      "Sample size for purpose 2, car av. 0: 91\n",
      "Sample size for purpose 2, car av. 1: 2000\n",
      "Sample size for purpose 3, car av. 0: 55\n",
      "[10:14:40] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 3, car av. 1: 2000\n",
      "Sample size for purpose 4, car av. 0: 351\n",
      "[10:16:51] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 4, car av. 1: 2000\n",
      "[10:17:47] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 6, car av. 0: 605\n",
      "[10:19:39] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 6, car av. 1: 2000\n",
      "[10:20:57] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 7, car av. 0: 41\n",
      "[10:27:47] < Warning >   Initial point not feasible. It will be projected onto the feasible domain.\n",
      "Sample size for purpose 7, car av. 1: 2000\n"
     ]
    }
   ],
   "source": [
    "# Run the estimation by purpose\n",
    "# and by car ownership\n",
    "results = []\n",
    "for p in [1,2,3,4,6,7]:\n",
    "    for car in [0,1]:\n",
    "        mask = (df['PURPOSE']==p) & (df['CAR_AV']==car)\n",
    "        num_inter = len(df.loc[(df['D']!=zone_dict['STAY']) & (df['O']!=df['D']) & mask])\n",
    "        num_inter = min(num_inter, 2000)\n",
    "        database = db.Database('MiD2017', pd.concat([\n",
    "            df.loc[(df['D']!=zone_dict['STAY']) & (df['O']!=df['D']) & mask].sample(num_inter) # inter-zonal trips\n",
    "            #df.loc[(df['D']!=zone_dict['STAY']) & (df['O']==df['D']) & mask].sample(num_inter) # inner-zonal trips\n",
    "            #df.loc[(df['D']==zone_dict['STAY']) & mask].sample(num_inter) # stay observations\n",
    "        ]))\n",
    "        print('Sample size for purpose {}, car av. {}: {}'.format(\n",
    "            p, car, database.getSampleSize()))\n",
    "        #nl = models.lognested(V_D[p][car], None, urban_nests, D)\n",
    "        mnl = models.loglogit(V_D[p][car], None, D)\n",
    "        formulas = {'loglike': mnl, 'weight': W_GEW}\n",
    "        model = bio.BIOGEME(database, formulas)\n",
    "        model.modelName = str(p) + '_' + str(car) # Name it\n",
    "        results.append(model.estimate(algoParameters={'proportionAnalyticalHessian': 0.5})\n",
    "                      ) # Estimation\n",
    "        output = results[-1].getEstimatedParameters()\n",
    "        # Add results to the Excel file\n",
    "        for key, val in results[-1].getGeneralStatistics().items():\n",
    "            output.loc[key] = [val[0], val[1]] + ['' for i in range(len(output.columns)-2)]\n",
    "        output.to_excel(writer, sheet_name=model.modelName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2154.2,\n",
       " -5270.4,\n",
       " -589.7,\n",
       " -6976.6,\n",
       " -497.8,\n",
       " -4799.6,\n",
       " -2280.3,\n",
       " -4762.6,\n",
       " -4304.6,\n",
       " -7255.0,\n",
       " -157.6,\n",
       " -5649.0]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# final log likelihoods\n",
    "'''\n",
    "using simple logs of attraction variables plus log(PopDens) and the CC\n",
    "or the Daly (1982) version with sum of attraction variables in the log\n",
    "                      PopDens  Prio1  Prio2  Prio3  All3   Daly3  DalyDist   DalyCCSq DalyDist2 DalyDistUrb DalyDistLogDist\n",
    "commuting, car av. 0: -2552.4, -2467  -2467  -2467  -2467  -2467  -2138      -2308    -2131     -2129       -1946\n",
    "commuting, car av. 1: -6284.0, -6075  -6622  -6920  -6449  -6100  -4669      -6823    -5125     -4797       -5446\n",
    "business , car av. 0: -690.8,  -653   -653   -653   -653   -653   -590       -606     -4736     -589        -589\n",
    "business , car av. 1: -8732.7, -8230  -8001  -8309  -8182  -8078  -7463      -8244    -9827     -6849       -6998\n",
    "education, car av. 0: -627.4,  -627   -627   -627   -621   -627   -483       -435     -482      -480        -432\n",
    "education, car av. 1: -8004.0, -7583  -7678  -7701  -7661  -7539  -4757      -7561    -63078    -4852       -4596\n",
    "buy/exec., car av. 0: -2797.0, -2743  -2768  -2736  -2730  -2736  -2278      -2729    -2115     -2277       -2260\n",
    "buy/exec., car av. 1: -6605.9, -6671  -7123  -6899  -6446  -7056  -4683      -6253    -4836     -4716       -5001\n",
    "leisure  , car av. 0: -4656.5, -4552  -4576  -4561  -4546  -4547  -4304      -4545    -4089     -4303       -4303\n",
    "leisure  , car av. 1: -9258.1, -8627  -8720  -8552  -8498  -9368  -7317      -8734    -6219     -6828       -6503\n",
    "accompany, car av. 0: -176.9,  -167   -171   -169   -166   -169   -157       -161     -157      -157        -156\n",
    "accompany, car av. 1: -8613.2  -8484  -8178  -8442  -8461  -8747  -5593      -8309    -5038     -6072       -5247\n",
    "correct beta signs  : false    true   true   true   notEdu true   noCarFalse false    false     false       false\n",
    "suffic. significant : true     true   true   true   true   true   true       false    false     false       true\n",
    " '''\n",
    "[np.round(r.getGeneralStatistics()['Final log likelihood'][0],1) for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size for purpose 1, car av. 0: 712\n",
      "Sample size for purpose 1, car av. 1: 4000\n",
      "Sample size for purpose 2, car av. 0: 182\n",
      "Sample size for purpose 2, car av. 1: 4000\n",
      "Sample size for purpose 3, car av. 0: 110\n",
      "Sample size for purpose 3, car av. 1: 4000\n",
      "Sample size for purpose 4, car av. 0: 702\n",
      "Sample size for purpose 4, car av. 1: 4000\n",
      "Sample size for purpose 6, car av. 0: 1210\n",
      "Sample size for purpose 6, car av. 1: 4000\n",
      "Sample size for purpose 7, car av. 0: 82\n",
      "Sample size for purpose 7, car av. 1: 4000\n"
     ]
    }
   ],
   "source": [
    "# Run the estimation for stay/go choice\n",
    "writer = pd.ExcelWriter(input_path + 'estimation_results_go.xls', engine='xlsxwriter')\n",
    "results_go = []\n",
    "for p in [1,2,3,4,6,7]:\n",
    "    for car in [0,1]:\n",
    "        mask = (df['PURPOSE']==p) & (df['CAR_AV']==car)\n",
    "        num_inter = len(df.loc[(df['D']!=zone_dict['STAY']) & (df['O']!=df['D']) & mask])\n",
    "        num_inter = min(num_inter, 2000)\n",
    "        database = db.Database('MiD2017', pd.concat([\n",
    "            df.loc[(df['D']!=zone_dict['STAY']) & (df['O']!=df['D']) & mask].sample(num_inter), # inter-zonal trips\n",
    "            df.loc[(df['D']!=zone_dict['STAY']) & (df['O']==df['D']) & mask].sample(num_inter) # inner-zonal trips\n",
    "            #df.loc[(df['D']==zone_dict['STAY']) & mask].sample(num_inter) # stay observations\n",
    "        ]))\n",
    "        print('Sample size for purpose {}, car av. {}: {}'.format(\n",
    "            p, car, database.getSampleSize()))\n",
    "        #nl = models.lognested(V_GO[p][car], None, go_nest, GO)\n",
    "        mnl = models.loglogit(V_GO[p][car], None, GO)\n",
    "        formulas = {'loglike': mnl, 'weight': W_GEW}\n",
    "        model = bio.BIOGEME(database, formulas)\n",
    "        model.modelName = str(p) + '_' + str(car) # Name it\n",
    "        results_go.append(model.estimate()) # Estimation\n",
    "        output = results_go[-1].getEstimatedParameters()\n",
    "        # Add results to the Excel file\n",
    "        for key, val in results_go[-1].getGeneralStatistics().items():\n",
    "            output.loc[key] = [val[0], val[1]] + ['' for i in range(len(output.columns)-2)]\n",
    "            \n",
    "        # Simulate results\n",
    "        inner = models.loglogit(V_GO[p][car], None, 1)\n",
    "        inter = models.loglogit(V_GO[p][car], None, 2)\n",
    "        simulate = {'Prob. inner-zonal': inner, 'Prob. inter-zonal': inter}\n",
    "        data_sim = db.Database('MiD2017', pd.concat([\n",
    "            df.loc[(df['D']!=zone_dict['STAY']) & (df['O']!=df['D']) & mask].sample(num_inter), # inter-zonal trips\n",
    "            df.loc[(df['D']!=zone_dict['STAY']) & (df['O']==df['D']) & mask].sample(num_inter) # inner-zonal trips\n",
    "            #df.loc[(df['D']==zone_dict['STAY']) & mask].sample(num_inter) # stay observations\n",
    "        ]))\n",
    "        biosim = bio.BIOGEME(data_sim, simulate)\n",
    "        simresults = biosim.simulate(results_go[-1].getBetaValues())\n",
    "        output.loc[list(simulate.keys())[0]] = simresults.mean()[0]\n",
    "        output.loc[list(simulate.keys())[1]] = simresults.mean()[1]\n",
    "        \n",
    "        output.to_excel(writer, sheet_name=model.modelName)\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
