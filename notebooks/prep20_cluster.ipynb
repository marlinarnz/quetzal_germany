{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # for automation and parallelisation\n",
    "manual, scenario = (True, 'base') if 'ipykernel' in sys.argv[0] else (False, sys.argv[1])\n",
    "if manual:\n",
    "    %matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely import geometry\n",
    "from geopy.distance import geodesic\n",
    "from tqdm import tqdm\n",
    "import shapely.speedups\n",
    "from quetzal.model import stepmodel\n",
    "from quetzal.io import excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the transport network.\n",
    "## Saves PT networks with clustered nodes.\n",
    "## Needs PT networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '../input_static/'\n",
    "output_path = '../output/'\n",
    "model_path = '../model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scenario parameters\n",
    "params = excel.read_var(file='../input/parameters.xls', scenario=scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bus = stepmodel.read_json(input_path + 'de_pt_network_bus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm = stepmodel.read_json(input_path + 'de_pt_network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to use the right zones\n",
    "try:\n",
    "    z = stepmodel.read_json(model_path + scenario + '/' + 'de_zones')\n",
    "except FileNotFoundError:\n",
    "    z = stepmodel.read_json(model_path + 'base' + '/' + 'de_zones')\n",
    "#z.zones['FID'] = z.zones['NUTS_ID']\n",
    "sm.zones = gpd.GeoDataFrame(z.zones)\n",
    "bus.zones = gpd.GeoDataFrame(z.zones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add reactivate links and nodes to the network, if policy is set\n",
    "if params['network_reactivation']['inclusion_reasons']:\n",
    "    print(f\"starting network reactivation for scenario {scenario}\")\n",
    "    \n",
    "    # Load rail network reactivation links and nodes\n",
    "    reac_links = gpd.read_file(input_path + '/de_rail_reactivation/links.geojson')\n",
    "    reac_nodes = gpd.read_file(input_path + '/de_rail_reactivation/nodes.geojson')\n",
    "    \n",
    "    # Adjust format to sm.links\n",
    "    reac_links.drop('index', axis=1, inplace=True)\n",
    "    reac_links.index = ['rs_reac_'+str(i) for i in range(len(reac_links))]\n",
    "    nodes_new_index = ['rs_reac_'+str(i) for i in range(len(reac_nodes))]\n",
    "    node_dict = dict(zip(reac_nodes['index'], nodes_new_index))\n",
    "    reac_links['a'] = reac_links['a'].map(node_dict)\n",
    "    reac_links['b'] = reac_links['b'].map(node_dict)\n",
    "    reac_nodes.index = nodes_new_index\n",
    "    reac_nodes['route_type'] = 'rail_short'\n",
    "    reac_links['route_id'] = reac_links['trip_id']\n",
    "    \n",
    "    # Commpute headways from scenario settings\n",
    "    reac_nodes['urbanisation'] = np.nan\n",
    "    shapely.speedups.enable()\n",
    "    for u in sm.zones['urbanisation'].unique():\n",
    "        reac_nodes.loc[reac_nodes['geometry'].within(\n",
    "            sm.zones.loc[sm.zones['urbanisation']==u, 'geometry'].unary_union\n",
    "        ), 'urbanisation'] = u\n",
    "    reac_nodes.loc[reac_nodes['urbanisation'].isna(), 'urbanisation'] = 3\n",
    "    reac_links['urbanisation'] = reac_links['a'].map(reac_nodes['urbanisation']).astype(int)\n",
    "    reac_links['headway'] = np.nan\n",
    "    for u in sm.zones['urbanisation'].unique():\n",
    "        reac_links.loc[reac_links['urbanisation']==u, 'headway'] = params['max_headway_urban'+str(u)]['rail_short']\n",
    "\n",
    "    # Filter for reasons\n",
    "    reasons = params['network_reactivation']['inclusion_reasons']\n",
    "    if isinstance(reasons, str):\n",
    "        reasons = [s.strip() for s in reasons.split(';')]\n",
    "        if reasons == list('ABCDEFGHI'):\n",
    "            filtered_links = reac_links\n",
    "            filtered_nodes = reac_nodes\n",
    "        else:\n",
    "            filtered_links = reac_links.loc[reac_links['agency_id'].apply(\n",
    "                lambda s: len( set(s.split(';')).intersection(set(reasons)) ) > 0)]\n",
    "            filtered_nodes = reac_nodes.loc[list(set(list(filtered_links['a']) + list(filtered_links['b'])))]\n",
    "        sm.links = pd.concat([sm.links, filtered_links])[list(sm.links.columns)]\n",
    "        sm.nodes = pd.concat([sm.nodes, filtered_nodes])[list(sm.nodes.columns)]\n",
    "        print('Added {} rail short links due to reactivation'.format(len(filtered_links)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test links and nodes for network integrity\n",
    "Neccessary for any further steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIRST: Set time of free-rider links\n",
    "# Default velocity of 14 m/s for rail links\n",
    "sm.links.loc[sm.links['time']==0, 'time'] = sm.links.loc[\n",
    "    sm.links['time']==0, 'geometry'].apply(\n",
    "        lambda l: int(geodesic(l.coords[0], l.coords[-1]).m)) / 14\n",
    "# 8 m/s for bus links\n",
    "bus.links.loc[bus.links['time']==0, 'time'] = bus.links.loc[\n",
    "    bus.links['time']==0, 'geometry'].apply(\n",
    "        lambda l: int(geodesic(l.coords[0], l.coords[-1]).m)) / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check nodeset integrity for later steps to work\n",
    "try:\n",
    "    sm.integrity_test_nodeset_consistency()\n",
    "except AssertionError:\n",
    "    print('Found {} orphan nodes'.format(len(sm.orphan_nodes)))\n",
    "    sm.nodes.drop(sm.orphan_nodes, inplace=True)\n",
    "    # Test integrity again\n",
    "    sm.integrity_test_nodeset_consistency()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sequences\n",
    "# Use an own function because quetzal's takes ages\n",
    "def test_sequences(trip):\n",
    "    assert len(trip)==trip['link_sequence'].max(), \\\n",
    "        'broken sequence in trip {}'.format(trip['trip_id'].unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix sequences\n",
    "# Use an own function because quetzal's takes ages\n",
    "def fix_sequences(trip):\n",
    "    if len(trip) > 1:\n",
    "        trip = trip.sort_values('link_sequence')\n",
    "        # Check link succession\n",
    "        ind = list(trip.index)\n",
    "        for i in range(len(trip.index) - 1):\n",
    "            try:\n",
    "                assert trip.loc[ind[i], 'b'] == trip.loc[ind[i+1], 'a'], \\\n",
    "                    'broken trip {}: stop {} has no successor link'.format(\n",
    "                        trip['trip_id'].unique()[0], trip.loc[ind[i], 'b'])\n",
    "            except AssertionError:\n",
    "                trip.loc[ind[i+1]:ind[-1], 'trip_id'] = \\\n",
    "                    trip.loc[ind[i+1]:ind[-1], 'trip_id'] + '_' + str(i)\n",
    "        # Repair sequences\n",
    "        if len(trip) != trip['link_sequence'].max():\n",
    "            trip['link_sequence'] = trip.groupby('trip_id')['link_sequence'].apply(\n",
    "                lambda t: [j for j in range(1, len(t.index)+1)]).sum()\n",
    "    return trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test and save broken sequences\n",
    "def test_sequences_save(trip):\n",
    "    if len(trip)!=trip['link_sequence'].max():\n",
    "        return list(trip.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sm.links.groupby('trip_id').apply(test_sequences)\n",
    "except AssertionError:\n",
    "    links = sm.links.groupby('trip_id').apply(fix_sequences).reset_index(level=0, drop=True)\n",
    "    links.groupby('trip_id').apply(test_sequences)\n",
    "    sm.links = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_seqs = bus.links.groupby('trip_id').apply(test_sequences_save)\n",
    "if not broken_seqs is None:\n",
    "    links = bus.links.loc[broken_seqs.loc[broken_seqs.notna()].sum()\n",
    "                         ].groupby('trip_id').apply(fix_sequences)\n",
    "    links.reset_index(level=0, drop=True, inplace=True)\n",
    "    links.groupby('trip_id').apply(test_sequences)\n",
    "    bus.links = pd.concat([bus.links.drop(broken_seqs.loc[broken_seqs.notna()].sum()), links])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(bus.nodes['route_type'].unique()) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.links.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.nodes.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster nodes\n",
    "\n",
    "Apply agglomerative clustering by zone with mode-specific distance thresholds. They determine the radius of any node cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge bus with other PT\n",
    "sm.links = pd.concat([sm.links, bus.links])\n",
    "sm.nodes = pd.concat([sm.nodes, bus.nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate local bus and tram, underground, ferry, funicular\n",
    "if 'subway' in sm.links['route_type'].unique():\n",
    "    sm.links['route_type_disagg'] = sm.links['route_type'].copy()\n",
    "    sm.links['route_type'] = sm.links['route_type'].replace(\n",
    "        {'tram': 'bus', 'subway': 'bus', 'ferry': 'bus', 'funicular': 'bus'})\n",
    "    sm.pt_route_types = list(sm.links['route_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the coordinate system to metric, cartesian\n",
    "# for the clustering algorithm to work\n",
    "sm = sm.change_epsg(3857, 'meter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster agglomeratively (with a cluster radius)\n",
    "# by route_type and zone\n",
    "nodes = []\n",
    "all_nodes = gpd.GeoDataFrame(sm.nodes.copy())\n",
    "links = []\n",
    "all_links = gpd.GeoDataFrame(sm.links.copy())\n",
    "sm.clustering_zones = sm.zones.copy()\n",
    "# Process each mode seperately and concatenate tables in the end\n",
    "for mode in sm.nodes['route_type'].unique():\n",
    "    sm.nodes = all_nodes.loc[all_nodes['route_type']==mode]\n",
    "    sm.links = all_links.loc[all_links['route_type']==mode]\n",
    "    dist_col = 'radius_'+mode\n",
    "    sm.clustering_zones[dist_col] = params['clustering'][dist_col]\n",
    "    \n",
    "    if sm.clustering_zones[dist_col].mean() != 0:\n",
    "        sm.preparation_clusterize_nodes(adaptive_clustering=True, distance_col=dist_col)\n",
    "        # Reindex with mode names\n",
    "        prefix = ''.join([s[0] for s in mode.split('_')]) + '_'\n",
    "        sm.nodes.index = [prefix + str(i) for i in sm.nodes.index]\n",
    "        sm.links['a'] = prefix + sm.links['a'].apply(str)\n",
    "        sm.links['b'] = prefix + sm.links['b'].apply(str)\n",
    "        # Set route_type\n",
    "        sm.nodes['route_type'] = mode\n",
    "        \n",
    "    nodes.append(sm.nodes.copy())\n",
    "    links.append(sm.links.copy())\n",
    "    \n",
    "sm.nodes = pd.concat(nodes)\n",
    "sm.links = pd.concat(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.nodes.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sm.nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the coordinate system back\n",
    "sm = sm.change_epsg(4326, 'degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster coverage\n",
    "if manual: gpd.GeoDataFrame(sm.node_parenthood).plot(color='blue', figsize=(9,9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of stops per cluster\n",
    "sm.node_clusters['count'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean\n",
    "And map nodes to zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map nodes to zones\n",
    "sm.nodes = gpd.GeoDataFrame(sm.nodes, crs=sm.epsg)\n",
    "shapely.speedups.enable()\n",
    "sm.nodes['FID'] = np.nan\n",
    "for _, zone in tqdm(sm.zones.iterrows(), total=sm.zones.shape[0]):\n",
    "    sm.nodes.loc[sm.nodes['geometry'].within(zone['geometry'].buffer(1e-3)), 'FID'] = zone['FID']\n",
    "len(sm.nodes.loc[sm.nodes['FID'].isna()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill zone information of outer stops\n",
    "assert len(sm.nodes[sm.nodes['FID'].isna()]) < 100\n",
    "sm.nodes.loc[sm.nodes['FID'].isna(), 'FID'] = sm.nodes.loc[sm.nodes['FID'].isna()].index.str[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repair link geometry\n",
    "geo_dict = sm.nodes['geometry'].to_dict()\n",
    "sm.links['geometry'] = [geometry.LineString((geo_dict[a].coords[0], geo_dict[b].coords[0]))\n",
    "                        for a,b in sm.links[['a', 'b']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop trips with different IDs but same stop sequences\n",
    "s = sm.links.groupby('trip_id').agg(tuple).drop_duplicates('a', keep='first')\n",
    "sm.links = sm.links.loc[sm.links['trip_id'].isin(s.index)]\n",
    "sm.links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove within-cluster links\n",
    "# Neglect time savings when small cluster distances applied\n",
    "def fix_circular(trip):\n",
    "    drops = trip.loc[trip['a']==trip['b']].index\n",
    "    if len(drops) > 0:\n",
    "        trip = trip.drop(drops).sort_values('link_sequence')\n",
    "        trip['link_sequence'] = list(range(1, len(trip)+1))\n",
    "    return trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.links = sm.links.groupby('trip_id').apply(fix_circular).reset_index(level=0, drop=True)\n",
    "sm.links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again: Drop trips with different IDs but same stop sequences\n",
    "s = sm.links.groupby('trip_id').agg(tuple).drop_duplicates('a', keep='first')\n",
    "sm.links = sm.links.loc[sm.links['trip_id'].isin(s.index)]\n",
    "sm.links.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrity tests\n",
    "try:\n",
    "    sm.integrity_test_nodeset_consistency()\n",
    "except AssertionError:\n",
    "    print('Number of orphan nodes: {}'.format(\n",
    "        len(sm.orphan_nodes)))\n",
    "    print('Number of missing nodes: {}'.format(\n",
    "        len(sm.missing_nodes)))\n",
    "sm.links.groupby('trip_id').apply(test_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop orphan nodes\n",
    "if len(sm.orphan_nodes) > 0: sm.nodes.drop(sm.orphan_nodes, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for circular trips (quetzal's function takes too long)\n",
    "def test_circular(trip):\n",
    "    if len(set(list(trip['a'])+list(trip['b']))) != len(trip)+1:\n",
    "        return trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test circular trips\n",
    "circular = sm.links.groupby('trip_id').apply(test_circular)\n",
    "print('Number of links in circular trips: '+str(len(circular)))\n",
    "print('Number of bus links in circular trips: '+str(len(circular.loc[circular['route_type']=='bus'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix circularity issue by splitting trips\n",
    "# Results in an additional interchange but prevents\n",
    "# dropping a large number of connections\n",
    "def fix_circular_split(trip):\n",
    "    def split_trip(trip, split_by):\n",
    "        split = [trip.index.get_loc(i) for i in trip.loc[trip[split_by].duplicated(keep=False)].index]\n",
    "        if len(split) >= 1:\n",
    "            trips = []\n",
    "            # First stops\n",
    "            trips.append(trip.iloc[: split[0]+1])\n",
    "            # Middle stops\n",
    "            for i in range(1, len(split)):\n",
    "                t = trip.iloc[split[i-1]+1 : split[i]]\n",
    "                t['trip_id'] = t['trip_id'] + '_' + str(i) + str(split_by)\n",
    "                t['link_sequence'] = list(range(1, len(t)+1))\n",
    "                trips.append(t)\n",
    "            # Last stops\n",
    "            t = trip.iloc[split[-1] :]\n",
    "            t['trip_id'] = t['trip_id'] + '_n' + str(split_by)\n",
    "            t['link_sequence'] = list(range(1, len(t)+1))\n",
    "            trips.append(t)\n",
    "            return pd.concat(trips)\n",
    "        else:\n",
    "            return trip\n",
    "    # Split duplicated b stops\n",
    "    trip = split_trip(trip, 'b')\n",
    "    # Split duplicated a stops\n",
    "    trip = trip.groupby('trip_id').apply(split_trip, 'a')\n",
    "    return trip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed = circular.reset_index(level='trip_id', drop=True).groupby('trip_id').apply(fix_circular_split)\n",
    "len(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that there are no circular lines anymore and the sequences are consecutive\n",
    "fixed = fixed.reset_index(level='trip_id', drop=True)\n",
    "fixed.groupby('trip_id').apply(test_sequences)\n",
    "still_circular = fixed.groupby('trip_id').apply(test_circular)\n",
    "if len(still_circular) > 0:\n",
    "    fixed.drop(still_circular.reset_index(level='trip_id', drop=True).index, inplace=True)\n",
    "len(fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace circular lines with fixed trips\n",
    "sm.links = sm.links.loc[~sm.links['trip_id'].isin(circular['trip_id'].unique())]\n",
    "sm.links = pd.concat([sm.links, fixed])\n",
    "len(sm.links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.links.drop(['disaggregated_a', 'disaggregated_b'], axis=1, inplace=True)\n",
    "assert len(sm.links.loc[sm.links.isna().any(axis=1)]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add bus service to ancilliary\n",
    "sm.agencies = pd.concat([sm.agencies, bus.agencies]).reset_index(drop=True)\n",
    "sm.pt_routes = pd.concat([sm.pt_routes, bus.pt_routes]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we have bus services in the same tables\n",
    "sm.pt_route_types = list(sm.links['route_type'].unique())\n",
    "sm.pt_route_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.links.loc[sm.links['route_type']=='rail_short'].sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast columns to integer\n",
    "cols = ['link_sequence', 'time', 'headway']\n",
    "sm.links[cols] = sm.links[cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce nodes table size by dropping stop names\n",
    "sm.nodes = sm.nodes[['FID', 'route_type', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split links in graph and auxiliary information\n",
    "# for file sizes being compatible with github's size limit\n",
    "cols = ['link_sequence', 'route_id', 'time', 'trip_id', 'headway']\n",
    "auxiliary = sm.links[cols]\n",
    "sm.links.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving model...\n",
    "sm.to_json(model_path + scenario + '/' + 'de_pt_network_agg',\n",
    "           only_attributes=['zones', 'links', 'nodes', 'pt_route_types'],\n",
    "           encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm.to_json(model_path + scenario + '/' + 'de_pt_network_ancillary',\n",
    "           only_attributes=['agencies', 'pt_routes', 'node_parenthood'],\n",
    "           encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save auxiliary information seperately\n",
    "auxiliary['index'] = auxiliary.index\n",
    "auxiliary.reset_index(drop=True, inplace=True)\n",
    "auxiliary.to_json(model_path + scenario + '/' + 'de_pt_network_agg/links_quetzaldata.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
